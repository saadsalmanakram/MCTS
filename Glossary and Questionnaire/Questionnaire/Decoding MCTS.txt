1- What is Monte Carlo Tree Search (MCTS)?

Ans- MCTS is a heuristic search algorithm that combines Monte Carlo methods with tree-based search techniques to handle complex decision-making and game-playing scenarios.

(-------------------------------------------------------------------------)

2- How does MCTS differ from traditional search algorithms?

Ans- Unlike traditional algorithms that exhaustively explore the search space, MCTS uses random sampling and statistical evaluation to focus on promising areas of the search space.

(-------------------------------------------------------------------------)

3- What are the four key steps in MCTS?

Ans- Selection, Expansion, Simulation, and Backpropagation.

(-------------------------------------------------------------------------)

4- What is the purpose of the Selection step in MCTS?

Ans- The Selection step involves traversing the tree from the root node to select the most promising child node using the Upper Confidence Bound (UCB) formula.

(-------------------------------------------------------------------------)

5- What happens during the Expansion step of MCTS?

Ans- A new child node is added to the tree at the node selected during the Selection step.

(-------------------------------------------------------------------------)

6- Describe the Simulation step in MCTS.

Ans- The Simulation step involves running random plays from the newly added node until a terminal state or predefined depth is reached.

(-------------------------------------------------------------------------)

7- What is Backpropagation in MCTS?

Ans- Backpropagation updates the nodes along the path from the newly added node to the root with the results of the simulation, including visit counts and win ratios.

(-------------------------------------------------------------------------)

8- What role does the Upper Confidence Bound (UCB) formula play in MCTS?

Ans- The UCB formula helps balance exploration and exploitation by evaluating nodes based on both their empirical mean and the number of visits.

(-------------------------------------------------------------------------)

9- Why is MCTS particularly useful for games with large search spaces?

Ans- MCTS effectively explores large and complex search spaces without requiring exhaustive exploration, making it suitable for strategic games with vast possibilities.

(-------------------------------------------------------------------------)

10- How does MCTS handle games with imperfect or unknown information?

Ans- MCTS relies on statistical sampling rather than complete knowledge, making it adaptable to scenarios with incomplete information.

(-------------------------------------------------------------------------)

11- What is the exploration-exploitation trade-off in MCTS?

Ans- It’s the balance between exploring new and potentially better moves and exploiting known promising moves to optimize decision-making.

(-------------------------------------------------------------------------)

12- What are some advantages of using MCTS?

Ans- MCTS can handle complex games, adapt to imperfect information, learn from simulations, and is scalable and parallelizable.

(-------------------------------------------------------------------------)

13- What are some disadvantages of MCTS?

Ans- MCTS can require significant memory, be computationally expensive, suffer from high variance, and face challenges in balancing exploration and exploitation.

(-------------------------------------------------------------------------)

14- How does MCTS achieve scalability and parallelization?

Ans- MCTS can be efficiently parallelized by running multiple simulations or tree searches concurrently, leveraging distributed computing resources.

(-------------------------------------------------------------------------)

15- What is the role of heuristics in MCTS?

Ans- Heuristics guide the search process, helping to prioritize nodes or actions, but their effectiveness depends on their quality and relevance to the problem domain.

(-------------------------------------------------------------------------)

16- Can MCTS be applied to domains outside of game playing?

Ans- Yes, MCTS principles are applicable to various domains such as planning, scheduling, and optimization, beyond traditional game playing.

(-------------------------------------------------------------------------)

17- What is the impact of the high variance in MCTS simulations?

Ans- High variance can lead to inconsistent action value estimations and introduce noise, affecting the reliability of the decision-making process.

(-------------------------------------------------------------------------)

18- How can MCTS be adapted to handle continuous action spaces?

Ans- MCTS can be extended with techniques like discretization or specialized algorithms to manage continuous action spaces effectively.

(-------------------------------------------------------------------------)

19- What techniques can be used to improve the sample efficiency of MCTS?

Ans- Techniques such as variance reduction, progressive widening, and more efficient simulation strategies can help improve sample efficiency.

(-------------------------------------------------------------------------)

20- What is overfitting in the context of MCTS?

Ans- Overfitting occurs when MCTS becomes too specialized in the early simulations, leading to suboptimal decisions. Techniques like exploration bonuses can help mitigate this.

(-------------------------------------------------------------------------)

21- How does MCTS balance exploration and exploitation?

Ans- MCTS uses the Upper Confidence Bound (UCB) formula, which prioritizes both high-value nodes (exploitation) and unexplored nodes (exploration).

(-------------------------------------------------------------------------)

22- What happens during the Selection phase of MCTS?

Ans- The algorithm selects the node to explore using a balance between exploration and exploitation based on the UCB formula.

(-------------------------------------------------------------------------)

23- What is the Expansion phase in MCTS?

Ans- The algorithm expands the game tree by adding a child node to the selected node.

(-------------------------------------------------------------------------)

24- What occurs in the Simulation phase of MCTS?

Ans- A random playout is conducted from the new node to a terminal state (win, lose, or draw).

(-------------------------------------------------------------------------)

25- What are combinatorial games in the context of MCTS?

Ans- These are two-player games with perfect information, such as chess, Go, or tic-tac-toe.

(-------------------------------------------------------------------------)

26- What is a game tree?

Ans- A game tree represents all possible game states, with nodes as states and edges as possible moves.

(-------------------------------------------------------------------------)

27- How does MCTS handle large branching factors in games?

Ans- MCTS focuses on promising branches by prioritizing exploration and uses random simulations to avoid exhaustively expanding the entire game tree.

(-------------------------------------------------------------------------)

28- What are the advantages of MCTS?

Ans- MCTS is domain-agnostic, can be halted anytime, and adapts asymmetrically to the search space.

(-------------------------------------------------------------------------)

29- What are the disadvantages of MCTS?

Ans- It requires a large amount of memory, may lack reliability due to insufficient exploration, and needs many iterations for effective results.

(-------------------------------------------------------------------------)

30- What does the term ‘anytime algorithm’ mean in MCTS?

Ans- MCTS can be stopped at any point and still return the best current estimate without completing the full search.

(-------------------------------------------------------------------------)

31- Why is MCTS considered domain-agnostic?

Ans- MCTS doesn't need specific domain knowledge and only relies on legal moves and end conditions to make decisions.

(-------------------------------------------------------------------------)

32- What is asymmetric tree growth in MCTS?

Ans- MCTS grows the tree more in promising areas, focusing on regions that seem more relevant based on previous results.

(-------------------------------------------------------------------------)

33- Why is memory usage a challenge for MCTS?

Ans- The rapid growth of the search tree after several iterations requires significant memory resources.

(-------------------------------------------------------------------------)

34- What is the role of UCB (Upper Confidence Bound) in MCTS?

Ans- UCB helps in selecting the best node by balancing between nodes with high-value returns and unexplored nodes.

(-------------------------------------------------------------------------)

35- How does MCTS handle unexplored nodes?

Ans- MCTS assigns infinite value to unexplored nodes, ensuring each node gets visited at least once.

(-------------------------------------------------------------------------)

36- How does MCTS work in game simulation?

Ans- MCTS simulates possible game moves, evaluates the outcomes, and updates the tree to reflect the results, often used in board games like Go and Chess.

(-------------------------------------------------------------------------)

37- What is the role of the simulation step in MCTS?

Ans- The simulation step runs a random or biased simulation from the selected node to estimate the outcome.

(-------------------------------------------------------------------------)

38- Why is MCTS successful in games like Go?

Ans- MCTS narrows the gap between human and computer performance by effectively handling the high branching factor and depth of Go's game tree.

(-------------------------------------------------------------------------)

39- What is the significance of domain knowledge in MCTS?

Ans- MCTS requires little to no domain-specific knowledge, making it effective in a wide range of problems.

(-------------------------------------------------------------------------)

40- What is UCT in MCTS?

Ans- UCT (Upper Confidence Bounds for Trees) is a variation of MCTS that converges to the optimal minimax decision over time.

(-------------------------------------------------------------------------)

41- What are some common enhancements to MCTS?

Ans- Enhancements include techniques like RAVE (Rapid Action Value Estimation), progressive bias, and parallelization.

(-------------------------------------------------------------------------)

42- What is a Partially Observable Markov Decision Process (POMDP)?

Ans- A POMDP is a decision-making model where the agent cannot fully observe the current state, adding complexity to the problem.

(-------------------------------------------------------------------------)

43- What is the advantage of MCTS over depth-limited minimax search?

Ans- MCTS does not require intermediate state evaluations, only the value of terminal states after simulations.

(-------------------------------------------------------------------------)

44- What are some key domains where MCTS is applied?

Ans- MCTS is widely used in games, planning, optimization, and real-world decision-making problems.

(-------------------------------------------------------------------------)

45- Why is MCTS considered asymmetric?

Ans- MCTS incrementally builds a tree that is asymmetric, focusing more on promising branches rather than evenly expanding all areas.

(-------------------------------------------------------------------------)

46- What is progressive bias in MCTS?

Ans- Progressive bias guides the search by incorporating heuristic knowledge to prioritize certain actions.

(-------------------------------------------------------------------------)

47- How does MCTS differ from traditional minimax algorithms?

Ans- MCTS uses random sampling to explore the search space, whereas minimax evaluates every possible move up to a certain depth.

(-------------------------------------------------------------------------)

48- What is the role of bandit algorithms in MCTS?

Ans- Bandit algorithms like UCB help in balancing the exploration-exploitation tradeoff in MCTS.

(-------------------------------------------------------------------------)

49- How has MCTS been extended for multi-player games?

Ans- MCTS has been adapted for multi-player scenarios using variations like Coalition Reduction and Ensemble UCT.

(-------------------------------------------------------------------------)

50- What is a multi-armed bandit problem?

Ans- It is a sequential decision problem where one must choose from multiple actions to maximize cumulative reward, balancing exploration and exploitation.

(-------------------------------------------------------------------------)

51- What is the exploration-exploitation dilemma?

Ans- It is the challenge of choosing between exploiting the action believed to be optimal and exploring potentially better but less tried options.

(-------------------------------------------------------------------------)

52- What is regret in the context of bandit problems?

Ans- Regret is the difference between the expected reward of the best action and the expected reward of the chosen actions over time.

(-------------------------------------------------------------------------)

53- What is the Upper Confidence Bound (UCB) in bandit algorithms?

Ans- UCB is a strategy that balances exploration and exploitation by selecting the action with the highest upper confidence bound on its reward.

(-------------------------------------------------------------------------)

54- What does the UCB1 formula consist of?

Ans- UCB1 combines the average reward of an arm with an exploration term that scales with the logarithm of total plays and inversely with the number of times the arm was played.

(-------------------------------------------------------------------------)

55- Why is UCB1 effective for bandit problems?

Ans- It provides a logarithmic bound on regret and ensures a balance between trying new actions and exploiting known high-reward actions.

(-------------------------------------------------------------------------)

56- What is the role of Monte Carlo simulations in MCTS?

Ans- They estimate action values by running random playouts and progressively refining these estimates as more simulations are performed.

(-------------------------------------------------------------------------)

57- Why is UCB1 a good choice for MCTS?

Ans- It balances the exploration-exploitation tradeoff efficiently and leads to logarithmic regret growth, ensuring robust performance in tree search.

(-------------------------------------------------------------------------)

58- What makes MCTS an aheuristic algorithm?

Ans- MCTS does not require domain-specific knowledge, making it adaptable across various domains.

(-------------------------------------------------------------------------)

59- Why is MCTS preferred over minimax in games like Go?

Ans- MCTS handles large branching factors effectively, where minimax struggles without reliable heuristics.

(-------------------------------------------------------------------------)

60- What is the trade-off of using domain-specific knowledge in MCTS?

Ans- Domain-specific knowledge reduces the number of simulations but also decreases result variance.

(-------------------------------------------------------------------------)

61- How does MCTS lead to asymmetric tree growth?

Ans- MCTS favors more promising nodes, leading to a skewed, asymmetric tree over time.

(-------------------------------------------------------------------------)

62- Why might MCTS outperform minimax in certain domains?

Ans- MCTS excels when there are no reliable heuristics, especially in games with large trees and fewer trap states.

(-------------------------------------------------------------------------)

63- In what type of game tree structures does UCT outperform minimax?

Ans- UCT outperforms minimax in bounded trees with a single optimal action per state.

(-------------------------------------------------------------------------)

64- What is the difference between MCTS and UCT?

Ans- UCT is a variation of MCTS that uses UCB for tree selection.

(-------------------------------------------------------------------------)

65- What is Flat UCB in the context of MCTS?

Ans- Flat UCB treats leaf nodes as a multi-armed bandit problem, avoiding tree growth.

(-------------------------------------------------------------------------)

66- How does the Bandit Algorithm for Smooth Trees (BAST) improve upon UCT?

Ans- BAST focuses on expanding optimal branches, ignoring suboptimal ones with high confidence.

(-------------------------------------------------------------------------)

67- How is MCTS related to Temporal Difference Learning (TDL)?

Ans- MCTS estimates temporary state values, while TDL learns long-term state values.

(-------------------------------------------------------------------------)

68- What is the purpose of combining MCTS with Temporal Difference Learning?

Ans- Combining MCTS with TDL allows leveraging heuristic value functions to improve tree and simulation policies.

(-------------------------------------------------------------------------)

69- What is Single-Player MCTS (SP-MCTS)?

Ans- SP-MCTS adds a third term to the UCB formula to manage variance in single-player games.

(-------------------------------------------------------------------------)

70- What is the goal of the Bandit-Based Active Learner (BAAL)?

Ans- BAAL minimizes generalization error in sparse data scenarios by approximating the optimal active learning policy.

(-------------------------------------------------------------------------)

71- What problem does Feature UCT Selection (FUSE) solve?

Ans- FUSE adapts UCT to the combinatorial optimization problem of feature selection.

(-------------------------------------------------------------------------)

72- What is Multi-Agent MCTS?

Ans- Multi-Agent MCTS extends the standard MCTS by using multiple simulation policies, improving search space exploration.

(-------------------------------------------------------------------------)

73- How does Multi-Agent MCTS improve playing strength?

Ans- By leveraging interactions between different agents with varied heuristics, it leads to better exploration and stronger play.

(-------------------------------------------------------------------------)

74- What is the challenge in Multi-Agent MCTS?

Ans- Finding the right set of agents with properties that improve playing strength is computationally intensive.

(-------------------------------------------------------------------------)

75- What is Ensemble UCT?

Ans- Ensemble UCT runs multiple UCT instances in parallel and combines their results for more robust decision-making.

(-------------------------------------------------------------------------)

76- How does Ensemble UCT compare to plain UCT?

Ans- Ensemble UCT often outperforms plain UCT given the same number of total iterations.

(-------------------------------------------------------------------------)

77- How is MCTS suited to real-time games?

Ans- MCTS can handle real-time constraints well due to its anytime nature and asymmetric tree exploration.

(-------------------------------------------------------------------------)

78- What is determinization in the context of MCTS?

Ans- Determinization transforms a stochastic game into a deterministic one by fixing chance events, making it easier to analyze.

(-------------------------------------------------------------------------)

79- How does Hindsight Optimisation (HOP) relate to MCTS?

Ans- HOP uses a determinization approach to provide upper bounds on expected rewards, aiding decision-making in stochastic games.

(-------------------------------------------------------------------------)

80- What is Sparse UCT?

Ans- Sparse UCT expands MCTS for stochastic outcomes, creating multiple child nodes for each possible outcome of a move.

(-------------------------------------------------------------------------)

81- What is Information Set UCT (ISUCT)?

Ans- ISUCT adapts MCTS to work with games of imperfect information by operating on trees of information sets.

(-------------------------------------------------------------------------)

82- How does Multiple MCTS (MMCTS) differ from standard MCTS?

Ans- MMCTS builds separate trees for each player, updating them simultaneously to better reflect each player's information.

(-------------------------------------------------------------------------)

83- What is UCT+ and how does it differ from regular UCT?

Ans- UCT+ replaces opponent decision nodes with chance nodes and selects actions based on reward averages and standard errors.

(-------------------------------------------------------------------------)

84- What is Monte Carlo α-β (MCαβ)?

Ans- MCαβ combines MCTS with traditional α-β pruning to enhance decision-making with shallow searches.

(-------------------------------------------------------------------------)

85- What is Monte Carlo Counterfactual Regret (MCCFR)?

Ans- MCCFR minimizes regret in games of imperfect information by sampling game tree paths and computing counterfactual regrets.

(-------------------------------------------------------------------------)

86- How does inference and opponent modeling enhance MCTS?

Ans- Opponent modeling helps infer hidden information, improving decision-making in games of imperfect information.

(-------------------------------------------------------------------------)

87- What challenge do simultaneous moves pose for MCTS?

Ans- Simultaneous moves create hidden information, requiring special algorithms like EXP3 to handle uncertainty effectively.

(-------------------------------------------------------------------------)

88- What is Meta-MCTS?

Ans- Meta-MCTS replaces the default policy with a nested MCTS, where Quasi Best-First focuses on exploitation and Beta Distribution Sampling favors exploration.

(-------------------------------------------------------------------------)

89- What are the two versions of Meta-MCTS?

Ans- The two versions are Quasi Best-First (exploitation-focused) and Beta Distribution Sampling (exploration-focused).

(-------------------------------------------------------------------------)

89- How does Meta-MCTS improve MOGO's performance in Go?

Ans- By generating better opening books using nested MCTS, Meta-MCTS enhances the playing strength of MOGO in 9×9 Go.

(-------------------------------------------------------------------------)

90- What is Heuristically Guided Swarm Tree Search (HGSTS)?

Ans- HGSTS conducts a breadth-first search in the game tree, using heuristics and UCT for prioritizing and expanding nodes.

(-------------------------------------------------------------------------)

91- What is Forward Search Sparse Sampling (FSSS)?

Ans- FSSS replaces known policies with sample-based planners to address UCT’s computational inefficiencies and achieve sample efficiency.

(-------------------------------------------------------------------------)

92- How does Bayesian FSSS (BFS3) extend FSSS?

Ans- BFS3 approaches Bayes-optimality by increasing the computational budget, linking learning and planning.

(-------------------------------------------------------------------------)

93- What is Threshold Ascent for Graphs (TAG)?

Ans- TAG maximizes an objective function over DAGs using k-armed bandits, exploring nodes with random simulations.

(-------------------------------------------------------------------------)

94- In which domain has TAG shown superior performance?

Ans- TAG has outperformed traditional methods in optimizing automatic performance tuning with DFT and FFT transforms.

(-------------------------------------------------------------------------)

95- What is Rapidly-exploring Random Trees (RRTs)?

Ans- RRTs explore search spaces by incrementally expanding a tree toward unexplored areas, akin to MCTS but focused on space exploration.

(-------------------------------------------------------------------------)

96- How do RRTs relate to MCTS?

Ans- Both share concepts like tree structure and exploration driven by random actions, often enhanced by heuristics.

(-------------------------------------------------------------------------)

97- What is the UNLEO algorithm?

Ans- UNLEO is a UCT-based heuristic approximation of an optimal optimization algorithm, grounded in NFL and CFL theorems.

(-------------------------------------------------------------------------)

98- What makes Bayesian inference expensive in UNLEO?

Ans- The complexity of evaluating objective functions is tackled by using UCT to make Bayesian inference computationally feasible.

(-------------------------------------------------------------------------)

99- What is UCTSAT?

Ans- UCTSAT applies UCT to solve satisfiability problems in conjunctive normal form (CNF) using various assignment strategies.

(-------------------------------------------------------------------------)

100- What are the variations of UCTSAT?

Ans- UCTSATcp (random assignment), UCTSATsbs (sequential assignment), and UCTSATh (heuristic-based playouts) are its main variations.

(-------------------------------------------------------------------------)

101- What is ρUCT?

Ans- ρUCT generalizes UCT by approximating a finite horizon expectimax operation using a sparse search tree of decision and chance nodes.

(-------------------------------------------------------------------------)

102- What is MC-AIXA, and how does it relate to ρUCT?

Ans- MC-AIXA, built using ρUCT, approaches optimal performance across various problem domains by applying this generalization of UCT.

(-------------------------------------------------------------------------)

103- What are Monte Carlo Random Walks (MRW)?

Ans- MRW selectively build search trees through random walks, focusing on local search to solve planning problems efficiently.

(-------------------------------------------------------------------------)

104- How does MRW-LTS differ from standard MCTS?

Ans- MRW-LTS extends Monte Carlo Random Walks by concentrating on local search more than global exploration in planning problems.

(-------------------------------------------------------------------------)

105- What is the MHSP algorithm?

Ans- MHSP is a planning algorithm that replaces random simulations in MCTS with heuristic evaluations and uses average rewards for node selection.

(-------------------------------------------------------------------------)

106- What makes MHSP an anytime algorithm?

Ans- MHSP can generate partial plans even before finding a complete solution, providing useful intermediate results.

(-------------------------------------------------------------------------)

107- What are the two main categories of MCTS tree policy enhancements?

Ans- Domain Independent and Domain Dependent.

(-------------------------------------------------------------------------)

108- What characterizes domain-independent MCTS enhancements?

Ans- They can be applied to any domain without prior knowledge and typically offer small improvements.

(-------------------------------------------------------------------------)

109- What is the focus of domain-dependent MCTS enhancements?

Ans- They are specific to particular domains, using prior knowledge or exploiting unique domain characteristics.

(-------------------------------------------------------------------------)

110- What is the role of bandit-based methods in MCTS?

Ans- They are central to node selection in the tree policy and help balance exploration and exploitation.

(------------------------------------------------------------------------)

111- What does the UCB1-Tuned algorithm modify in the UCB1 formula?

Ans- It tunes the upper confidence bound by incorporating variance.

(------------------------------------------------------------------------)

112- What is the advantage of Bayesian UCT over standard UCT?

Ans- It allows more accurate estimation of node values and uncertainties using a Bayesian framework.

(------------------------------------------------------------------------)

113- What problem does the EXP3 algorithm address in MCTS?

Ans- It addresses games with partial observability and simultaneous moves.

(------------------------------------------------------------------------)

114- What is Hierarchical Optimistic Optimisation for Trees (HOOT)?

Ans- It generalizes stochastic bandits to overcome the discrete action limitation of UCT.

(------------------------------------------------------------------------)

115- What is the purpose of First Play Urgency (FPU) in MCTS?

Ans- It scores unvisited nodes to encourage early exploitation.

(------------------------------------------------------------------------)

116- What are decisive and anti-decisive moves in MCTS?

Ans- Decisive moves lead to a win, and anti-decisive moves prevent the opponent from making a decisive move.

(------------------------------------------------------------------------)

117- How do move groups improve MCTS performance in certain games?

Ans- They reduce the branching factor by grouping similar moves and using UCB1 to select between groups.

(------------------------------------------------------------------------)

118- What is a transposition in the context of MCTS?

Ans- It occurs when the same state/action pair is reached through different paths, allowing information reuse across the search tree.

(------------------------------------------------------------------------)

119- What is the benefit of using transposition tables in MCTS?

Ans- They store statistics for edges in the DAG, improving search efficiency by sharing information.

(------------------------------------------------------------------------)

120- What is the progressive bias technique in MCTS?

Ans- It incorporates heuristic knowledge into the selection process, favoring nodes with better heuristic values when statistics are unreliable.

(------------------------------------------------------------------------)

121- What is Monte Carlo Paraphrase Generation (MCPG)?

Ans- MCPG is an MCTS variant where selection is based on the maximum reachable score rather than the average score for paraphrasing natural language statements.

(------------------------------------------------------------------------)

122- How does MCPG differ from plain UCT?

Ans- MCPG uses the maximum reachable score for state selection, unlike UCT which uses the average score.

(------------------------------------------------------------------------)

123- What is search seeding in MCTS?

Ans- Search seeding initializes nodes with heuristic-based statistics to enhance search efficiency and reduce the need for simulations.

(------------------------------------------------------------------------)

124- How does seeding impact the performance of an MCTS?

Ans- Seeding can increase playing strength by providing better initial estimates, potentially reducing the number of required simulations.

(------------------------------------------------------------------------)

125- Why is parameter tuning important in MCTS?

Ans- Parameter tuning, such as adjusting exploration constants, optimizes MCTS performance for specific domains and enhances overall efficiency.

(------------------------------------------------------------------------)

126- What is an example of a parameter that might be tuned in MCTS?

Ans- The UCT exploration constant Cp is a commonly tuned parameter.

(------------------------------------------------------------------------)

127- How can parameter tuning be automated in MCTS?

Ans- Techniques like Cross-Entropy Methods, dynamic exploration, and neural networks can be used for automated parameter tuning.

(------------------------------------------------------------------------)

128- What is the history heuristic in MCTS?

Ans- It leverages past move data to improve action selection and simulation policies within the MCTS framework.

(------------------------------------------------------------------------)

129- What are the two levels of the history heuristic?

Ans- The tree-tree level and the tree-playout level.

(------------------------------------------------------------------------)

130- What is Progressive History in MCTS?

Ans- Progressive History combines Progressive Bias with history heuristics by using history scores in the progressive bias calculation.

(------------------------------------------------------------------------)

131- For which type of games has Progressive History been shown to perform well?

Ans- Multi-player board games.

(------------------------------------------------------------------------)

132- What does AMAF stand for?

Ans- All Moves As First.

(------------------------------------------------------------------------)

133- What is the purpose of the AMAF heuristic?

Ans- AMAF updates statistics for all actions encountered during a simulation as if they were the first move.

(------------------------------------------------------------------------)

134- How does α-AMAF differ from standard AMAF?

Ans- α-AMAF blends UCT and AMAF scores, adjusting α to balance their contributions.

(------------------------------------------------------------------------)

135- What is Rapid Action Value Estimation (RAVE)?

Ans- RAVE is an AMAF enhancement that uses a decreasing α value to combine AMAF and UCT scores.

(------------------------------------------------------------------------)

136- What is the difference between RAVE and Killer RAVE?

Ans- Killer RAVE focuses on using only the most important moves for updates, as opposed to RAVE which uses all moves.

(------------------------------------------------------------------------)

137- What is RAVE-max?

Ans- RAVE-max is a robust extension of RAVE that uses a variable α value based on visit counts.

(------------------------------------------------------------------------)

138- What is PoolRAVE?

Ans- PoolRAVE involves selecting moves from a pool of top RAVE-rated moves with a certain probability.

(------------------------------------------------------------------------)

139- What is a game-theoretic enhancement in MCTS?

Ans- It involves backing up known game-theoretic values during backpropagation to improve reward estimates for non-terminal nodes.

(------------------------------------------------------------------------)

140- How does backing up game-theoretic values affect MCTS?

Ans- It provides accurate reward estimates for non-terminal states, improving the overall search quality.

(------------------------------------------------------------------------)

141- What is Proof-Number Search (PNS) and how is it used in MCTS?

Ans- PNS is a technique for proving game-theoretic values by prioritizing nodes that can be proven with the fewest children; it's used in MCTS to enhance the game tree exploration by proving game states.

(------------------------------------------------------------------------)

142- How does Proof-Number Search (PNS) handle non-terminal states?

Ans- Non-terminal states are proven wins if at least one child is a proven win, or proven losses if all children are proven losses.

(------------------------------------------------------------------------)

143- How does MCTS-Solver integrate PNS with UCB selection?

Ans- MCTS-Solver uses PNS to prove game values and applies normal UCB selection if the parent node is visited enough times; otherwise, a simulation policy guides node selection.

(------------------------------------------------------------------------)

Monte Carlo Proof-Number Search (MC-PNS)

144- What distinguishes MC-PNS from standard PNS?

Ans- MC-PNS combines proof-number search with Monte Carlo simulations to evaluate nodes, making the proof of game-theoretic values faster and with fewer nodes.

(------------------------------------------------------------------------)

145- What is Score Bounded MCTS and how does it improve the search process?

Ans- Score Bounded MCTS uses optimistic and pessimistic bounds on a node's score to converge to the estimated score and reduce simulations by focusing on nodes with defined bounds.

(------------------------------------------------------------------------)

146- How do optimistic and pessimistic bounds function in Score Bounded MCTS?

Ans- These bounds help prove nodes by narrowing the range of possible scores, thus optimizing action selection and improving efficiency in games like Go and Connect Four.

(------------------------------------------------------------------------)

147- What is the role of move pruning in MCTS?

Ans- Move pruning removes suboptimal moves from the search tree, allowing MCTS to focus on more promising choices and improve efficiency.

(------------------------------------------------------------------------)

148- What is the difference between soft pruning and hard pruning in MCTS?

Ans- Soft pruning removes moves that may be revisited later, while hard pruning eliminates moves that will never be considered, potentially sacrificing optimality for efficiency.

(------------------------------------------------------------------------)

149- What is Progressive Unpruning and how does it benefit MCTS?

Ans- Progressive Unpruning gradually reintroduces moves that were initially pruned, ensuring that all moves are eventually considered, improving search efficiency and performance.

(------------------------------------------------------------------------)

150- How does Progressive Widening differ from Progressive Unpruning?

Ans- Progressive Widening controls the expansion of child nodes in MCTS based on the number of visits, helping balance exploration and exploitation in continuous or stochastic problems.

(------------------------------------------------------------------------)

151- How does Absolute Pruning differ from Relative Pruning in MCTS?

Ans- Absolute Pruning keeps only the most visited action, while Relative Pruning uses a visit threshold to determine if the most visited action will remain the most visited, enhancing search focus and accuracy.

(------------------------------------------------------------------------)

152- How does pruning with domain knowledge enhance MCTS performance?

Ans- Pruning with domain knowledge removes inferior moves based on specific game insights, leading to significant performance improvements in programs like LINGO and MOHEX.

(------------------------------------------------------------------------)

153- What is the default simulation policy in MCTS, and what is its main drawback?

Ans- The default simulation policy selects actions randomly, which ensures coverage of different areas of the search space but may not reflect realistic gameplay.

(------------------------------------------------------------------------)

154- What are "heavy playouts" in the context of MCTS?

Ans- Heavy playouts involve incorporating domain knowledge into simulations to make them more realistic, as opposed to purely random play.

(------------------------------------------------------------------------)

155- How does a rule-based simulation policy improve MCTS?

Ans- A rule-based simulation policy uses domain-specific rules to guide actions, aiming to enhance simulation efficiency and realism.

(------------------------------------------------------------------------)

156- What is Contextual Monte Carlo Search (CMCS), and how does it benefit simulations?

Ans- CMCS combines simulations that reach similar areas of the tree into tiles, using historical data to guide future simulations, improving efficiency and relevance.

(------------------------------------------------------------------------)

157- Explain the "Fill the Board" enhancement in MCTS.

Ans- Fill the Board involves selecting random board intersections to play, which speeds up board space occupation and can help apply strategic patterns earlier in the game.

(------------------------------------------------------------------------)

158- What is Move-Average Sampling Technique (MAST), and how does it work?

Ans- MAST uses average reward values for actions to bias selection towards more promising moves during simulations, using a Gibbs distribution based on historical data.

(------------------------------------------------------------------------)

159- How does Predicate-Average Sampling Technique (PAST) differ from MAST?

Ans- PAST maintains average values for predicate/action pairs rather than just actions, allowing it to consider contextual information and bias simulations accordingly.

(------------------------------------------------------------------------)

160- What is Feature-Average Sampling Technique (FAST), and when is it useful?

Ans-  FAST extracts game features to learn their importance and bias simulations, improving performance in games defined with the Game Description Language (GDL).

(------------------------------------------------------------------------)

161- How can history heuristics be applied to simulations in MCTS?

Ans- History heuristics use historical performance of moves to guide simulation choices, potentially improving effectiveness by reusing successful moves from past simulations.

(------------------------------------------------------------------------)

162- What role does an evaluation function play in improving simulation policies?

Ans- An evaluation function helps avoid bad moves early in simulations and guides towards better moves later, enhancing the quality of simulated gameplay.

(------------------------------------------------------------------------)

163- Describe simulation balancing in MCTS and its potential benefits.

Ans- Simulation balancing uses gradient descent to adjust simulation policies, aiming to produce balanced play rather than necessarily stronger play.

(------------------------------------------------------------------------)

164- What is the Last Good Reply (LGR) enhancement, and how does it operate?

Ans- LGR stores successful responses to moves and reuses them in future simulations, improving policy by focusing on historically successful replies.

(------------------------------------------------------------------------)

165- How does the Last Good Reply with Forgetting (LGRF) enhancement differ from LGR?

Ans- LGRF adds "forgetting" by removing stored replies that lead to losses in recent simulations, aiming to refine and adapt the simulation policy over time.

(------------------------------------------------------------------------)

166- What is the role of patterns in MCTS simulations, and how do they improve performance?

Ans- Patterns detect specific board configurations to guide simulations, making them more realistic and effective by mimicking strategic moves seen in human play.

(------------------------------------------------------------------------)

167- What is the purpose of weighting simulation results in MCTS?

Ans- Weighting simulation results gives more importance to later and shorter simulations, which tend to be more accurate.

(------------------------------------------------------------------------)

168- How does the score bonus enhancement modify the backpropagation process in MCTS?

Ans- It backpropagates rewards in a range [0, γ] for losses and [γ, 1] for wins to differentiate between strong and weak wins.

(------------------------------------------------------------------------)

169- What is the decaying reward modification in MCTS backpropagation?

Ans- It multiplies the reward value by a constant 0<γ≤1 to weight early wins more heavily than later ones.

(------------------------------------------------------------------------)

170- What are transposition table updates in MCTS and their variations?

Ans- They share information between nodes corresponding to the same state, with UCT1, UCT2, and UCT3 being different strategies for handling transpositions.

(------------------------------------------------------------------------)

171- What is leaf parallelisation in MCTS?

Ans- Leaf parallelisation involves performing multiple simultaneous simulations at each leaf node to improve statistics.

(------------------------------------------------------------------------)

172- How does root parallelisation differ from leaf parallelisation in MCTS?

Ans- Root parallelisation builds multiple MCTS trees simultaneously from the root, while leaf parallelisation focuses on the leaf nodes.

(------------------------------------------------------------------------)

173- What is tree parallelisation in MCTS?

Ans- Tree parallelisation involves multiple threads performing MCTS simulations on the same tree, with mechanisms to handle simultaneous access and updates.

(------------------------------------------------------------------------)

174- Describe the UCT-Treesplit approach in MCTS parallelisation.

Ans- UCT-Treesplit distributes both work and memory load evenly across multiple computational nodes in distributed memory systems.

(------------------------------------------------------------------------)

175- What is the Master-Slave algorithm for MCTS and its effect?

Ans- The Master-Slave algorithm involves a master node distributing tasks to slave nodes, improving performance with increasing parallelisation.

(------------------------------------------------------------------------)

176- How does multi-threaded MCTS without locks compare to locked approaches?

Ans- Multi-threaded MCTS without locks generally scales better across threads compared to approaches that require locks.

(------------------------------------------------------------------------)

177- What consistency issues can arise with heavily modified MCTS algorithms?

Ans- They may lead to incorrect behavior as computational power increases, such as failing to identify winning moves in complex scenarios.

(------------------------------------------------------------------------)

178- How can parameters of game trees affect MCTS performance?

Ans- Measurable parameters of game trees can predict MCTS success in various games, such as trick-taking vs. poker-like card games.

(------------------------------------------------------------------------)

179- What metrics are useful for comparing MCTS enhancements?

Ans- Metrics include win rate, Elo ratings, iterations per second, and memory usage, with the choice depending on the enhancement's purpose.

(------------------------------------------------------------------------)

180- What is a playout in MCTS?

Ans- A playout is a simulation of a game from a given node to a terminal state to estimate the value of that node.

(------------------------------------------------------------------------)

181- What is the Leftmost Path Problem in MCTS?

Ans- It involves constructing a binary tree and scoring based on the number of moves on the leftmost path, with leaf scores highly correlated to tree structure.

(------------------------------------------------------------------------)

182- How does the Left Move Problem differ from the Leftmost Path Problem?

Ans- In the Left Move Problem, the score is based on the number of moves to the left, resulting in less correlation with tree structure compared to the Leftmost Path Problem.

(------------------------------------------------------------------------)

183- What is Morpion Solitaire and how is it solved using MCTS?

Ans- Morpion Solitaire is a puzzle where players color vertices to form lines of five. MCTS has been used to improve solutions, with notable achievements including solving non-touching versions with up to 177 moves.

(------------------------------------------------------------------------)

184- How did Nested Monte Carlo Search (NMCS) improve Morpion Solitaire solutions?

Ans- NMCS enhanced the solution by exploring more possible moves, leading to higher move counts in solutions, such as 80 moves for the non-touching version and 177 for the touching version.

(------------------------------------------------------------------------)

