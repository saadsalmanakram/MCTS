1- What is Monte Carlo Tree Search (MCTS)?

Ans- MCTS is a heuristic search algorithm that combines Monte Carlo methods with tree-based search techniques to handle complex decision-making and game-playing scenarios.

(-------------------------------------------------------------------------)

2- How does MCTS differ from traditional search algorithms?

Ans- Unlike traditional algorithms that exhaustively explore the search space, MCTS uses random sampling and statistical evaluation to focus on promising areas of the search space.

(-------------------------------------------------------------------------)

3- What are the four key steps in MCTS?

Ans- Selection, Expansion, Simulation, and Backpropagation.

(-------------------------------------------------------------------------)

4- What is the purpose of the Selection step in MCTS?

Ans- The Selection step involves traversing the tree from the root node to select the most promising child node using the Upper Confidence Bound (UCB) formula.

(-------------------------------------------------------------------------)

5- What happens during the Expansion step of MCTS?

Ans- A new child node is added to the tree at the node selected during the Selection step.

(-------------------------------------------------------------------------)

6- Describe the Simulation step in MCTS.

Ans- The Simulation step involves running random plays from the newly added node until a terminal state or predefined depth is reached.

(-------------------------------------------------------------------------)

7- What is Backpropagation in MCTS?

Ans- Backpropagation updates the nodes along the path from the newly added node to the root with the results of the simulation, including visit counts and win ratios.

(-------------------------------------------------------------------------)

8- What role does the Upper Confidence Bound (UCB) formula play in MCTS?

Ans- The UCB formula helps balance exploration and exploitation by evaluating nodes based on both their empirical mean and the number of visits.

(-------------------------------------------------------------------------)

9- Why is MCTS particularly useful for games with large search spaces?

Ans- MCTS effectively explores large and complex search spaces without requiring exhaustive exploration, making it suitable for strategic games with vast possibilities.

(-------------------------------------------------------------------------)

10- How does MCTS handle games with imperfect or unknown information?

Ans- MCTS relies on statistical sampling rather than complete knowledge, making it adaptable to scenarios with incomplete information.

(-------------------------------------------------------------------------)

11- What is the exploration-exploitation trade-off in MCTS?

Ans- It’s the balance between exploring new and potentially better moves and exploiting known promising moves to optimize decision-making.

(-------------------------------------------------------------------------)

12- What are some advantages of using MCTS?

Ans- MCTS can handle complex games, adapt to imperfect information, learn from simulations, and is scalable and parallelizable.

(-------------------------------------------------------------------------)

13- What are some disadvantages of MCTS?

Ans- MCTS can require significant memory, be computationally expensive, suffer from high variance, and face challenges in balancing exploration and exploitation.

(-------------------------------------------------------------------------)

14- How does MCTS achieve scalability and parallelization?

Ans- MCTS can be efficiently parallelized by running multiple simulations or tree searches concurrently, leveraging distributed computing resources.

(-------------------------------------------------------------------------)

15- What is the role of heuristics in MCTS?

Ans- Heuristics guide the search process, helping to prioritize nodes or actions, but their effectiveness depends on their quality and relevance to the problem domain.

(-------------------------------------------------------------------------)

16- Can MCTS be applied to domains outside of game playing?

Ans- Yes, MCTS principles are applicable to various domains such as planning, scheduling, and optimization, beyond traditional game playing.

(-------------------------------------------------------------------------)

17- What is the impact of the high variance in MCTS simulations?

Ans- High variance can lead to inconsistent action value estimations and introduce noise, affecting the reliability of the decision-making process.

(-------------------------------------------------------------------------)

18- How can MCTS be adapted to handle continuous action spaces?

Ans- MCTS can be extended with techniques like discretization or specialized algorithms to manage continuous action spaces effectively.

(-------------------------------------------------------------------------)

19- What techniques can be used to improve the sample efficiency of MCTS?

Ans- Techniques such as variance reduction, progressive widening, and more efficient simulation strategies can help improve sample efficiency.

(-------------------------------------------------------------------------)

20- What is overfitting in the context of MCTS?

Ans- Overfitting occurs when MCTS becomes too specialized in the early simulations, leading to suboptimal decisions. Techniques like exploration bonuses can help mitigate this.

(-------------------------------------------------------------------------)

21- How does MCTS balance exploration and exploitation?

Ans- MCTS uses the Upper Confidence Bound (UCB) formula, which prioritizes both high-value nodes (exploitation) and unexplored nodes (exploration).

(-------------------------------------------------------------------------)

22- What happens during the Selection phase of MCTS?

Ans- The algorithm selects the node to explore using a balance between exploration and exploitation based on the UCB formula.

(-------------------------------------------------------------------------)

23- What is the Expansion phase in MCTS?

Ans- The algorithm expands the game tree by adding a child node to the selected node.

(-------------------------------------------------------------------------)

24- What occurs in the Simulation phase of MCTS?

Ans- A random playout is conducted from the new node to a terminal state (win, lose, or draw).

(-------------------------------------------------------------------------)

25- What are combinatorial games in the context of MCTS?

Ans- These are two-player games with perfect information, such as chess, Go, or tic-tac-toe.

(-------------------------------------------------------------------------)

26- What is a game tree?

Ans- A game tree represents all possible game states, with nodes as states and edges as possible moves.

(-------------------------------------------------------------------------)

27- How does MCTS handle large branching factors in games?

Ans- MCTS focuses on promising branches by prioritizing exploration and uses random simulations to avoid exhaustively expanding the entire game tree.

(-------------------------------------------------------------------------)

28- What are the advantages of MCTS?

Ans- MCTS is domain-agnostic, can be halted anytime, and adapts asymmetrically to the search space.

(-------------------------------------------------------------------------)

29- What are the disadvantages of MCTS?

Ans- It requires a large amount of memory, may lack reliability due to insufficient exploration, and needs many iterations for effective results.

(-------------------------------------------------------------------------)

30- What does the term ‘anytime algorithm’ mean in MCTS?

Ans- MCTS can be stopped at any point and still return the best current estimate without completing the full search.

(-------------------------------------------------------------------------)

31- Why is MCTS considered domain-agnostic?

Ans- MCTS doesn't need specific domain knowledge and only relies on legal moves and end conditions to make decisions.

(-------------------------------------------------------------------------)

32- What is asymmetric tree growth in MCTS?

Ans- MCTS grows the tree more in promising areas, focusing on regions that seem more relevant based on previous results.

(-------------------------------------------------------------------------)

33- Why is memory usage a challenge for MCTS?

Ans- The rapid growth of the search tree after several iterations requires significant memory resources.

(-------------------------------------------------------------------------)

34- What is the role of UCB (Upper Confidence Bound) in MCTS?

Ans- UCB helps in selecting the best node by balancing between nodes with high-value returns and unexplored nodes.

(-------------------------------------------------------------------------)

35- How does MCTS handle unexplored nodes?

Ans- MCTS assigns infinite value to unexplored nodes, ensuring each node gets visited at least once.

(-------------------------------------------------------------------------)

36- How does MCTS work in game simulation?

Ans- MCTS simulates possible game moves, evaluates the outcomes, and updates the tree to reflect the results, often used in board games like Go and Chess.

(-------------------------------------------------------------------------)

37- What is the role of the simulation step in MCTS?

Ans- The simulation step runs a random or biased simulation from the selected node to estimate the outcome.

(-------------------------------------------------------------------------)

38- Why is MCTS successful in games like Go?

Ans- MCTS narrows the gap between human and computer performance by effectively handling the high branching factor and depth of Go's game tree.

(-------------------------------------------------------------------------)

39- What is the significance of domain knowledge in MCTS?

Ans- MCTS requires little to no domain-specific knowledge, making it effective in a wide range of problems.

(-------------------------------------------------------------------------)

40- What is UCT in MCTS?

Ans- UCT (Upper Confidence Bounds for Trees) is a variation of MCTS that converges to the optimal minimax decision over time.

(-------------------------------------------------------------------------)

41- What are some common enhancements to MCTS?

Ans- Enhancements include techniques like RAVE (Rapid Action Value Estimation), progressive bias, and parallelization.

(-------------------------------------------------------------------------)

42- What is a Partially Observable Markov Decision Process (POMDP)?

Ans- A POMDP is a decision-making model where the agent cannot fully observe the current state, adding complexity to the problem.

(-------------------------------------------------------------------------)

43- What is the advantage of MCTS over depth-limited minimax search?

Ans- MCTS does not require intermediate state evaluations, only the value of terminal states after simulations.

(-------------------------------------------------------------------------)

44- What are some key domains where MCTS is applied?

Ans- MCTS is widely used in games, planning, optimization, and real-world decision-making problems.

(-------------------------------------------------------------------------)

45- Why is MCTS considered asymmetric?

Ans- MCTS incrementally builds a tree that is asymmetric, focusing more on promising branches rather than evenly expanding all areas.

(-------------------------------------------------------------------------)

46- What is progressive bias in MCTS?

Ans- Progressive bias guides the search by incorporating heuristic knowledge to prioritize certain actions.

(-------------------------------------------------------------------------)

47- How does MCTS differ from traditional minimax algorithms?

Ans- MCTS uses random sampling to explore the search space, whereas minimax evaluates every possible move up to a certain depth.

(-------------------------------------------------------------------------)

48- What is the role of bandit algorithms in MCTS?

Ans- Bandit algorithms like UCB help in balancing the exploration-exploitation tradeoff in MCTS.

(-------------------------------------------------------------------------)

49- How has MCTS been extended for multi-player games?

Ans- MCTS has been adapted for multi-player scenarios using variations like Coalition Reduction and Ensemble UCT.

(-------------------------------------------------------------------------)

50- What is a multi-armed bandit problem?

Ans- It is a sequential decision problem where one must choose from multiple actions to maximize cumulative reward, balancing exploration and exploitation.

(-------------------------------------------------------------------------)

51- What is the exploration-exploitation dilemma?

Ans- It is the challenge of choosing between exploiting the action believed to be optimal and exploring potentially better but less tried options.

(-------------------------------------------------------------------------)

52- What is regret in the context of bandit problems?

Ans- Regret is the difference between the expected reward of the best action and the expected reward of the chosen actions over time.

(-------------------------------------------------------------------------)

53- What is the Upper Confidence Bound (UCB) in bandit algorithms?

Ans- UCB is a strategy that balances exploration and exploitation by selecting the action with the highest upper confidence bound on its reward.

(-------------------------------------------------------------------------)

54- What does the UCB1 formula consist of?

Ans- UCB1 combines the average reward of an arm with an exploration term that scales with the logarithm of total plays and inversely with the number of times the arm was played.

(-------------------------------------------------------------------------)

55- Why is UCB1 effective for bandit problems?

Ans- It provides a logarithmic bound on regret and ensures a balance between trying new actions and exploiting known high-reward actions.

(-------------------------------------------------------------------------)

56- What is the role of Monte Carlo simulations in MCTS?

Ans- They estimate action values by running random playouts and progressively refining these estimates as more simulations are performed.

(-------------------------------------------------------------------------)

57- Why is UCB1 a good choice for MCTS?

Ans- It balances the exploration-exploitation tradeoff efficiently and leads to logarithmic regret growth, ensuring robust performance in tree search.

(-------------------------------------------------------------------------)

58- What makes MCTS an aheuristic algorithm?

Ans- MCTS does not require domain-specific knowledge, making it adaptable across various domains.

(-------------------------------------------------------------------------)

59- Why is MCTS preferred over minimax in games like Go?

Ans- MCTS handles large branching factors effectively, where minimax struggles without reliable heuristics.

(-------------------------------------------------------------------------)

60- What is the trade-off of using domain-specific knowledge in MCTS?

Ans- Domain-specific knowledge reduces the number of simulations but also decreases result variance.

(-------------------------------------------------------------------------)

61- How does MCTS lead to asymmetric tree growth?

Ans- MCTS favors more promising nodes, leading to a skewed, asymmetric tree over time.

(-------------------------------------------------------------------------)

62- Why might MCTS outperform minimax in certain domains?

Ans- MCTS excels when there are no reliable heuristics, especially in games with large trees and fewer trap states.

(-------------------------------------------------------------------------)

63- In what type of game tree structures does UCT outperform minimax?

Ans- UCT outperforms minimax in bounded trees with a single optimal action per state.

(-------------------------------------------------------------------------)

64- What is the difference between MCTS and UCT?

Ans- UCT is a variation of MCTS that uses UCB for tree selection.

(-------------------------------------------------------------------------)

65- What is Flat UCB in the context of MCTS?

Ans- Flat UCB treats leaf nodes as a multi-armed bandit problem, avoiding tree growth.

(-------------------------------------------------------------------------)

66- How does the Bandit Algorithm for Smooth Trees (BAST) improve upon UCT?

Ans- BAST focuses on expanding optimal branches, ignoring suboptimal ones with high confidence.

(-------------------------------------------------------------------------)

67- How is MCTS related to Temporal Difference Learning (TDL)?

Ans- MCTS estimates temporary state values, while TDL learns long-term state values.

(-------------------------------------------------------------------------)

68- What is the purpose of combining MCTS with Temporal Difference Learning?

Ans- Combining MCTS with TDL allows leveraging heuristic value functions to improve tree and simulation policies.

(-------------------------------------------------------------------------)

69- What is Single-Player MCTS (SP-MCTS)?

Ans- SP-MCTS adds a third term to the UCB formula to manage variance in single-player games.

(-------------------------------------------------------------------------)

70- What is the goal of the Bandit-Based Active Learner (BAAL)?

Ans- BAAL minimizes generalization error in sparse data scenarios by approximating the optimal active learning policy.

(-------------------------------------------------------------------------)

71- What problem does Feature UCT Selection (FUSE) solve?

Ans- FUSE adapts UCT to the combinatorial optimization problem of feature selection.

(-------------------------------------------------------------------------)

72- What is Multi-Agent MCTS?

Ans- Multi-Agent MCTS extends the standard MCTS by using multiple simulation policies, improving search space exploration.

(-------------------------------------------------------------------------)

73- How does Multi-Agent MCTS improve playing strength?

Ans- By leveraging interactions between different agents with varied heuristics, it leads to better exploration and stronger play.

(-------------------------------------------------------------------------)

74- What is the challenge in Multi-Agent MCTS?

Ans- Finding the right set of agents with properties that improve playing strength is computationally intensive.

(-------------------------------------------------------------------------)

75- What is Ensemble UCT?

Ans- Ensemble UCT runs multiple UCT instances in parallel and combines their results for more robust decision-making.

(-------------------------------------------------------------------------)

76- How does Ensemble UCT compare to plain UCT?

Ans- Ensemble UCT often outperforms plain UCT given the same number of total iterations.

(-------------------------------------------------------------------------)

77- How is MCTS suited to real-time games?

Ans- MCTS can handle real-time constraints well due to its anytime nature and asymmetric tree exploration.

(-------------------------------------------------------------------------)

78- What is determinization in the context of MCTS?

Ans- Determinization transforms a stochastic game into a deterministic one by fixing chance events, making it easier to analyze.

(-------------------------------------------------------------------------)

79- How does Hindsight Optimisation (HOP) relate to MCTS?

Ans- HOP uses a determinization approach to provide upper bounds on expected rewards, aiding decision-making in stochastic games.

(-------------------------------------------------------------------------)

80- What is Sparse UCT?

Ans- Sparse UCT expands MCTS for stochastic outcomes, creating multiple child nodes for each possible outcome of a move.

(-------------------------------------------------------------------------)

81- What is Information Set UCT (ISUCT)?

Ans- ISUCT adapts MCTS to work with games of imperfect information by operating on trees of information sets.

(-------------------------------------------------------------------------)

82- How does Multiple MCTS (MMCTS) differ from standard MCTS?

Ans- MMCTS builds separate trees for each player, updating them simultaneously to better reflect each player's information.

(-------------------------------------------------------------------------)

83- What is UCT+ and how does it differ from regular UCT?

Ans- UCT+ replaces opponent decision nodes with chance nodes and selects actions based on reward averages and standard errors.

(-------------------------------------------------------------------------)

84- What is Monte Carlo α-β (MCαβ)?

Ans- MCαβ combines MCTS with traditional α-β pruning to enhance decision-making with shallow searches.

(-------------------------------------------------------------------------)

85- What is Monte Carlo Counterfactual Regret (MCCFR)?

Ans- MCCFR minimizes regret in games of imperfect information by sampling game tree paths and computing counterfactual regrets.

(-------------------------------------------------------------------------)

86- How does inference and opponent modeling enhance MCTS?

Ans- Opponent modeling helps infer hidden information, improving decision-making in games of imperfect information.

(-------------------------------------------------------------------------)

87- What challenge do simultaneous moves pose for MCTS?

Ans- Simultaneous moves create hidden information, requiring special algorithms like EXP3 to handle uncertainty effectively.

(-------------------------------------------------------------------------)

88- What is Meta-MCTS?

Ans- Meta-MCTS replaces the default policy with a nested MCTS, where Quasi Best-First focuses on exploitation and Beta Distribution Sampling favors exploration.

(-------------------------------------------------------------------------)

89- What are the two versions of Meta-MCTS?

Ans- The two versions are Quasi Best-First (exploitation-focused) and Beta Distribution Sampling (exploration-focused).

(-------------------------------------------------------------------------)

89- How does Meta-MCTS improve MOGO's performance in Go?

Ans- By generating better opening books using nested MCTS, Meta-MCTS enhances the playing strength of MOGO in 9×9 Go.

(-------------------------------------------------------------------------)

90- What is Heuristically Guided Swarm Tree Search (HGSTS)?

Ans- HGSTS conducts a breadth-first search in the game tree, using heuristics and UCT for prioritizing and expanding nodes.

(-------------------------------------------------------------------------)

91- What is Forward Search Sparse Sampling (FSSS)?

Ans- FSSS replaces known policies with sample-based planners to address UCT’s computational inefficiencies and achieve sample efficiency.

(-------------------------------------------------------------------------)

92- How does Bayesian FSSS (BFS3) extend FSSS?

Ans- BFS3 approaches Bayes-optimality by increasing the computational budget, linking learning and planning.

(-------------------------------------------------------------------------)

93- What is Threshold Ascent for Graphs (TAG)?

Ans- TAG maximizes an objective function over DAGs using k-armed bandits, exploring nodes with random simulations.

(-------------------------------------------------------------------------)

94- In which domain has TAG shown superior performance?

Ans- TAG has outperformed traditional methods in optimizing automatic performance tuning with DFT and FFT transforms.

(-------------------------------------------------------------------------)

95- What is Rapidly-exploring Random Trees (RRTs)?

Ans- RRTs explore search spaces by incrementally expanding a tree toward unexplored areas, akin to MCTS but focused on space exploration.

(-------------------------------------------------------------------------)

96- How do RRTs relate to MCTS?

Ans- Both share concepts like tree structure and exploration driven by random actions, often enhanced by heuristics.

(-------------------------------------------------------------------------)

97- What is the UNLEO algorithm?

Ans- UNLEO is a UCT-based heuristic approximation of an optimal optimization algorithm, grounded in NFL and CFL theorems.

(-------------------------------------------------------------------------)

98- What makes Bayesian inference expensive in UNLEO?

Ans- The complexity of evaluating objective functions is tackled by using UCT to make Bayesian inference computationally feasible.

(-------------------------------------------------------------------------)

99- What is UCTSAT?

Ans- UCTSAT applies UCT to solve satisfiability problems in conjunctive normal form (CNF) using various assignment strategies.

(-------------------------------------------------------------------------)

100- What are the variations of UCTSAT?

Ans- UCTSATcp (random assignment), UCTSATsbs (sequential assignment), and UCTSATh (heuristic-based playouts) are its main variations.

(-------------------------------------------------------------------------)

101- What is ρUCT?

Ans- ρUCT generalizes UCT by approximating a finite horizon expectimax operation using a sparse search tree of decision and chance nodes.

(-------------------------------------------------------------------------)

102- What is MC-AIXA, and how does it relate to ρUCT?

Ans- MC-AIXA, built using ρUCT, approaches optimal performance across various problem domains by applying this generalization of UCT.

(-------------------------------------------------------------------------)

103- What are Monte Carlo Random Walks (MRW)?

Ans- MRW selectively build search trees through random walks, focusing on local search to solve planning problems efficiently.

(-------------------------------------------------------------------------)

104- How does MRW-LTS differ from standard MCTS?

Ans- MRW-LTS extends Monte Carlo Random Walks by concentrating on local search more than global exploration in planning problems.

(-------------------------------------------------------------------------)

105- What is the MHSP algorithm?

Ans- MHSP is a planning algorithm that replaces random simulations in MCTS with heuristic evaluations and uses average rewards for node selection.

(-------------------------------------------------------------------------)

106- What makes MHSP an anytime algorithm?

Ans- MHSP can generate partial plans even before finding a complete solution, providing useful intermediate results.

(-------------------------------------------------------------------------)

107- What are the two main categories of MCTS tree policy enhancements?

Ans- Domain Independent and Domain Dependent.

(-------------------------------------------------------------------------)

108- What characterizes domain-independent MCTS enhancements?

Ans- They can be applied to any domain without prior knowledge and typically offer small improvements.

(-------------------------------------------------------------------------)

109- What is the focus of domain-dependent MCTS enhancements?

Ans- They are specific to particular domains, using prior knowledge or exploiting unique domain characteristics.

(-------------------------------------------------------------------------)

110- What is the role of bandit-based methods in MCTS?

Ans- They are central to node selection in the tree policy and help balance exploration and exploitation.

(------------------------------------------------------------------------)

111- What does the UCB1-Tuned algorithm modify in the UCB1 formula?

Ans- It tunes the upper confidence bound by incorporating variance.

(------------------------------------------------------------------------)

112- What is the advantage of Bayesian UCT over standard UCT?

Ans- It allows more accurate estimation of node values and uncertainties using a Bayesian framework.

(------------------------------------------------------------------------)

113- What problem does the EXP3 algorithm address in MCTS?

Ans- It addresses games with partial observability and simultaneous moves.

(------------------------------------------------------------------------)

114- What is Hierarchical Optimistic Optimisation for Trees (HOOT)?

Ans- It generalizes stochastic bandits to overcome the discrete action limitation of UCT.

(------------------------------------------------------------------------)

115- What is the purpose of First Play Urgency (FPU) in MCTS?

Ans- It scores unvisited nodes to encourage early exploitation.

(------------------------------------------------------------------------)

116- What are decisive and anti-decisive moves in MCTS?

Ans- Decisive moves lead to a win, and anti-decisive moves prevent the opponent from making a decisive move.

(------------------------------------------------------------------------)

117- How do move groups improve MCTS performance in certain games?

Ans- They reduce the branching factor by grouping similar moves and using UCB1 to select between groups.

(------------------------------------------------------------------------)

118- What is a transposition in the context of MCTS?

Ans- It occurs when the same state/action pair is reached through different paths, allowing information reuse across the search tree.

(------------------------------------------------------------------------)

119- What is the benefit of using transposition tables in MCTS?

Ans- They store statistics for edges in the DAG, improving search efficiency by sharing information.

(------------------------------------------------------------------------)

120- What is the progressive bias technique in MCTS?

Ans- It incorporates heuristic knowledge into the selection process, favoring nodes with better heuristic values when statistics are unreliable.

(------------------------------------------------------------------------)

121- What is Monte Carlo Paraphrase Generation (MCPG)?

Ans- MCPG is an MCTS variant where selection is based on the maximum reachable score rather than the average score for paraphrasing natural language statements.

(------------------------------------------------------------------------)

122- How does MCPG differ from plain UCT?

Ans- MCPG uses the maximum reachable score for state selection, unlike UCT which uses the average score.

(------------------------------------------------------------------------)

123- What is search seeding in MCTS?

Ans- Search seeding initializes nodes with heuristic-based statistics to enhance search efficiency and reduce the need for simulations.

(------------------------------------------------------------------------)

124- How does seeding impact the performance of an MCTS?

Ans- Seeding can increase playing strength by providing better initial estimates, potentially reducing the number of required simulations.

(------------------------------------------------------------------------)

125- Why is parameter tuning important in MCTS?

Ans- Parameter tuning, such as adjusting exploration constants, optimizes MCTS performance for specific domains and enhances overall efficiency.

(------------------------------------------------------------------------)

126- What is an example of a parameter that might be tuned in MCTS?

Ans- The UCT exploration constant Cp is a commonly tuned parameter.

(------------------------------------------------------------------------)

127- How can parameter tuning be automated in MCTS?

Ans- Techniques like Cross-Entropy Methods, dynamic exploration, and neural networks can be used for automated parameter tuning.

(------------------------------------------------------------------------)

128- What is the history heuristic in MCTS?

Ans- It leverages past move data to improve action selection and simulation policies within the MCTS framework.

(------------------------------------------------------------------------)

129- What are the two levels of the history heuristic?

Ans- The tree-tree level and the tree-playout level.

(------------------------------------------------------------------------)

130- What is Progressive History in MCTS?

Ans- Progressive History combines Progressive Bias with history heuristics by using history scores in the progressive bias calculation.

(------------------------------------------------------------------------)

131- For which type of games has Progressive History been shown to perform well?

Ans- Multi-player board games.

(------------------------------------------------------------------------)

132- What does AMAF stand for?

Ans- All Moves As First.

(------------------------------------------------------------------------)

133- What is the purpose of the AMAF heuristic?

Ans- AMAF updates statistics for all actions encountered during a simulation as if they were the first move.

(------------------------------------------------------------------------)

134- How does α-AMAF differ from standard AMAF?

Ans- α-AMAF blends UCT and AMAF scores, adjusting α to balance their contributions.

(------------------------------------------------------------------------)

135- What is Rapid Action Value Estimation (RAVE)?

Ans- RAVE is an AMAF enhancement that uses a decreasing α value to combine AMAF and UCT scores.

(------------------------------------------------------------------------)

136- What is the difference between RAVE and Killer RAVE?

Ans- Killer RAVE focuses on using only the most important moves for updates, as opposed to RAVE which uses all moves.

(------------------------------------------------------------------------)

137- What is RAVE-max?

Ans- RAVE-max is a robust extension of RAVE that uses a variable α value based on visit counts.

(------------------------------------------------------------------------)

138- What is PoolRAVE?

Ans- PoolRAVE involves selecting moves from a pool of top RAVE-rated moves with a certain probability.

(------------------------------------------------------------------------)

139- What is a game-theoretic enhancement in MCTS?

Ans- It involves backing up known game-theoretic values during backpropagation to improve reward estimates for non-terminal nodes.

(------------------------------------------------------------------------)

140- How does backing up game-theoretic values affect MCTS?

Ans- It provides accurate reward estimates for non-terminal states, improving the overall search quality.

(------------------------------------------------------------------------)

141- What is Proof-Number Search (PNS) and how is it used in MCTS?

Ans- PNS is a technique for proving game-theoretic values by prioritizing nodes that can be proven with the fewest children; it's used in MCTS to enhance the game tree exploration by proving game states.

(------------------------------------------------------------------------)

142- How does Proof-Number Search (PNS) handle non-terminal states?

Ans- Non-terminal states are proven wins if at least one child is a proven win, or proven losses if all children are proven losses.

(------------------------------------------------------------------------)

143- How does MCTS-Solver integrate PNS with UCB selection?

Ans- MCTS-Solver uses PNS to prove game values and applies normal UCB selection if the parent node is visited enough times; otherwise, a simulation policy guides node selection.

(------------------------------------------------------------------------)

Monte Carlo Proof-Number Search (MC-PNS)

144- What distinguishes MC-PNS from standard PNS?

Ans- MC-PNS combines proof-number search with Monte Carlo simulations to evaluate nodes, making the proof of game-theoretic values faster and with fewer nodes.

(------------------------------------------------------------------------)

145- What is Score Bounded MCTS and how does it improve the search process?

Ans- Score Bounded MCTS uses optimistic and pessimistic bounds on a node's score to converge to the estimated score and reduce simulations by focusing on nodes with defined bounds.

(------------------------------------------------------------------------)

146- How do optimistic and pessimistic bounds function in Score Bounded MCTS?

Ans- These bounds help prove nodes by narrowing the range of possible scores, thus optimizing action selection and improving efficiency in games like Go and Connect Four.

(------------------------------------------------------------------------)

147- What is the role of move pruning in MCTS?

Ans- Move pruning removes suboptimal moves from the search tree, allowing MCTS to focus on more promising choices and improve efficiency.

(------------------------------------------------------------------------)

148- What is the difference between soft pruning and hard pruning in MCTS?

Ans- Soft pruning removes moves that may be revisited later, while hard pruning eliminates moves that will never be considered, potentially sacrificing optimality for efficiency.

(------------------------------------------------------------------------)

149- What is Progressive Unpruning and how does it benefit MCTS?

Ans- Progressive Unpruning gradually reintroduces moves that were initially pruned, ensuring that all moves are eventually considered, improving search efficiency and performance.

(------------------------------------------------------------------------)

150- How does Progressive Widening differ from Progressive Unpruning?

Ans- Progressive Widening controls the expansion of child nodes in MCTS based on the number of visits, helping balance exploration and exploitation in continuous or stochastic problems.

(------------------------------------------------------------------------)

151- How does Absolute Pruning differ from Relative Pruning in MCTS?

Ans- Absolute Pruning keeps only the most visited action, while Relative Pruning uses a visit threshold to determine if the most visited action will remain the most visited, enhancing search focus and accuracy.

(------------------------------------------------------------------------)

152- How does pruning with domain knowledge enhance MCTS performance?

Ans- Pruning with domain knowledge removes inferior moves based on specific game insights, leading to significant performance improvements in programs like LINGO and MOHEX.

(------------------------------------------------------------------------)

153- What is the default simulation policy in MCTS, and what is its main drawback?

Ans- The default simulation policy selects actions randomly, which ensures coverage of different areas of the search space but may not reflect realistic gameplay.

(------------------------------------------------------------------------)

154- What are "heavy playouts" in the context of MCTS?

Ans- Heavy playouts involve incorporating domain knowledge into simulations to make them more realistic, as opposed to purely random play.

(------------------------------------------------------------------------)

155- How does a rule-based simulation policy improve MCTS?

Ans- A rule-based simulation policy uses domain-specific rules to guide actions, aiming to enhance simulation efficiency and realism.

(------------------------------------------------------------------------)

156- What is Contextual Monte Carlo Search (CMCS), and how does it benefit simulations?

Ans- CMCS combines simulations that reach similar areas of the tree into tiles, using historical data to guide future simulations, improving efficiency and relevance.

(------------------------------------------------------------------------)

157- Explain the "Fill the Board" enhancement in MCTS.

Ans- Fill the Board involves selecting random board intersections to play, which speeds up board space occupation and can help apply strategic patterns earlier in the game.

(------------------------------------------------------------------------)

158- What is Move-Average Sampling Technique (MAST), and how does it work?

Ans- MAST uses average reward values for actions to bias selection towards more promising moves during simulations, using a Gibbs distribution based on historical data.

(------------------------------------------------------------------------)

159- How does Predicate-Average Sampling Technique (PAST) differ from MAST?

Ans- PAST maintains average values for predicate/action pairs rather than just actions, allowing it to consider contextual information and bias simulations accordingly.

(------------------------------------------------------------------------)

160- What is Feature-Average Sampling Technique (FAST), and when is it useful?

Ans-  FAST extracts game features to learn their importance and bias simulations, improving performance in games defined with the Game Description Language (GDL).

(------------------------------------------------------------------------)

161- How can history heuristics be applied to simulations in MCTS?

Ans- History heuristics use historical performance of moves to guide simulation choices, potentially improving effectiveness by reusing successful moves from past simulations.

(------------------------------------------------------------------------)

162- What role does an evaluation function play in improving simulation policies?

Ans- An evaluation function helps avoid bad moves early in simulations and guides towards better moves later, enhancing the quality of simulated gameplay.

(------------------------------------------------------------------------)

163- Describe simulation balancing in MCTS and its potential benefits.

Ans- Simulation balancing uses gradient descent to adjust simulation policies, aiming to produce balanced play rather than necessarily stronger play.

(------------------------------------------------------------------------)

164- What is the Last Good Reply (LGR) enhancement, and how does it operate?

Ans- LGR stores successful responses to moves and reuses them in future simulations, improving policy by focusing on historically successful replies.

(------------------------------------------------------------------------)

165- How does the Last Good Reply with Forgetting (LGRF) enhancement differ from LGR?

Ans- LGRF adds "forgetting" by removing stored replies that lead to losses in recent simulations, aiming to refine and adapt the simulation policy over time.

(------------------------------------------------------------------------)

166- What is the role of patterns in MCTS simulations, and how do they improve performance?

Ans- Patterns detect specific board configurations to guide simulations, making them more realistic and effective by mimicking strategic moves seen in human play.

(------------------------------------------------------------------------)

167- What is the purpose of weighting simulation results in MCTS?

Ans- Weighting simulation results gives more importance to later and shorter simulations, which tend to be more accurate.

(------------------------------------------------------------------------)

168- How does the score bonus enhancement modify the backpropagation process in MCTS?

Ans- It backpropagates rewards in a range [0, γ] for losses and [γ, 1] for wins to differentiate between strong and weak wins.

(------------------------------------------------------------------------)

169- What is the decaying reward modification in MCTS backpropagation?

Ans- It multiplies the reward value by a constant 0<γ≤1 to weight early wins more heavily than later ones.

(------------------------------------------------------------------------)

170- What are transposition table updates in MCTS and their variations?

Ans- They share information between nodes corresponding to the same state, with UCT1, UCT2, and UCT3 being different strategies for handling transpositions.

(------------------------------------------------------------------------)

171- What is leaf parallelisation in MCTS?

Ans- Leaf parallelisation involves performing multiple simultaneous simulations at each leaf node to improve statistics.

(------------------------------------------------------------------------)

172- How does root parallelisation differ from leaf parallelisation in MCTS?

Ans- Root parallelisation builds multiple MCTS trees simultaneously from the root, while leaf parallelisation focuses on the leaf nodes.

(------------------------------------------------------------------------)

173- What is tree parallelisation in MCTS?

Ans- Tree parallelisation involves multiple threads performing MCTS simulations on the same tree, with mechanisms to handle simultaneous access and updates.

(------------------------------------------------------------------------)

174- Describe the UCT-Treesplit approach in MCTS parallelisation.

Ans- UCT-Treesplit distributes both work and memory load evenly across multiple computational nodes in distributed memory systems.

(------------------------------------------------------------------------)

175- What is the Master-Slave algorithm for MCTS and its effect?

Ans- The Master-Slave algorithm involves a master node distributing tasks to slave nodes, improving performance with increasing parallelisation.

(------------------------------------------------------------------------)

176- How does multi-threaded MCTS without locks compare to locked approaches?

Ans- Multi-threaded MCTS without locks generally scales better across threads compared to approaches that require locks.

(------------------------------------------------------------------------)

177- What consistency issues can arise with heavily modified MCTS algorithms?

Ans- They may lead to incorrect behavior as computational power increases, such as failing to identify winning moves in complex scenarios.

(------------------------------------------------------------------------)

178- How can parameters of game trees affect MCTS performance?

Ans- Measurable parameters of game trees can predict MCTS success in various games, such as trick-taking vs. poker-like card games.

(------------------------------------------------------------------------)

179- What metrics are useful for comparing MCTS enhancements?

Ans- Metrics include win rate, Elo ratings, iterations per second, and memory usage, with the choice depending on the enhancement's purpose.

(------------------------------------------------------------------------)

180- What is a playout in MCTS?

Ans- A playout is a simulation of a game from a given node to a terminal state to estimate the value of that node.

(------------------------------------------------------------------------)

181- What is the Leftmost Path Problem in MCTS?

Ans- It involves constructing a binary tree and scoring based on the number of moves on the leftmost path, with leaf scores highly correlated to tree structure.

(------------------------------------------------------------------------)

182- How does the Left Move Problem differ from the Leftmost Path Problem?

Ans- In the Left Move Problem, the score is based on the number of moves to the left, resulting in less correlation with tree structure compared to the Leftmost Path Problem.

(------------------------------------------------------------------------)

183- What is Morpion Solitaire and how is it solved using MCTS?

Ans- Morpion Solitaire is a puzzle where players color vertices to form lines of five. MCTS has been used to improve solutions, with notable achievements including solving non-touching versions with up to 177 moves.

(------------------------------------------------------------------------)

184- How did Nested Monte Carlo Search (NMCS) improve Morpion Solitaire solutions?

Ans- NMCS enhanced the solution by exploring more possible moves, leading to higher move counts in solutions, such as 80 moves for the non-touching version and 177 for the touching version.

(------------------------------------------------------------------------)

185- What role did AMAF heuristic play in solving Morpion Solitaire?

Ans- The AMAF heuristic helped improve move selection efficiency by storing and reusing results from similar moves, contributing to achieving a record of 146 moves.

(------------------------------------------------------------------------)

186- How did Nested Rollout Policy Adaptation (NRPA) contribute to crossword puzzle construction?

Ans- NRPA was used to maximize word usage per puzzle, improving the design of crossword puzzles by exploring more possibilities in the construction process.

(------------------------------------------------------------------------)

187- How does MCTS handle Sudoku and Kakuro puzzles?

Ans- MCTS was used to solve Sudoku and Kakuro puzzles more efficiently than traditional methods, with faster solution rates for larger puzzles.

(------------------------------------------------------------------------)

188- What is CADIAPLAYER and its significance in GGP?

Ans- CADIAPLAYER is an MCTS-based GGP agent that won the AAAI GGP competitions in 2007 and 2008, demonstrating the effectiveness of UCT for general game playing.

(------------------------------------------------------------------------)

189- How did ARY improve upon CADIAPLAYER’s performance in GGP?

Ans- ARY used nested Monte Carlo search and transposition tables to improve performance, winning the 2009 and 2010 AAAI GGP competitions.

(------------------------------------------------------------------------)

190- What enhancements were added to CADIAPLAYER for improved performance?

Ans- Enhancements included MAST, TO-MAST, PAST, and RAVE techniques to improve performance across various games.

(------------------------------------------------------------------------)

191- How did domain-specific knowledge improve UCT performance in general games?

Ans- Domain-specific knowledge helped to evolve and generate better strategies, improving convergence rates and performance in general games.

(------------------------------------------------------------------------)

192- How is MCTS applied to Tron and what are its challenges?

Ans- MCTS was applied to Tron with specific modifications to handle self-entrapment and survival modes, though challenges include dealing with ineffective playouts.

(------------------------------------------------------------------------)

193- Describe the use of UCT in the Dead-End game.

Ans- UCT was used for the behavior of dogs in Dead-End, showing better performance with increased simulation time compared to flat Monte Carlo approaches.

(------------------------------------------------------------------------)

194- How did MCTS address Dynamic Difficulty Adjustment (DDA) in games?

Ans- MCTS was used to dynamically adjust difficulty levels by varying simulation times, showing how difficulty can impact performance in real-time games.

(------------------------------------------------------------------------)

195- What is the impact of MCTS on real-time strategy games like Wargus?

Ans- MCTS applied to Wargus showed promising results in tactical planning, outperforming baseline and human players despite lacking domain-specific knowledge.

(------------------------------------------------------------------------)

196- What is a common approach to handle the increased branching factor in nondeterministic games using MCTS?

Ans- Determinization, which involves sampling perfect information game instances.

(------------------------------------------------------------------------)

197- What advantage does UCT with a learned opponent model provide in Poker?

Ans- It significantly increases playing strength by biasing determinizations based on the opponent’s previous games.

(------------------------------------------------------------------------)

198- How does the application of hindsight optimization (HOP) benefit Klondike Solitaire?

Ans- HOP combined with UCT achieved a win rate more than twice that of human players.

(------------------------------------------------------------------------)

199- What is the primary challenge addressed by MCTS in games like Scotland Yard?

Ans- Handling imperfect information and fixed coalitions.

(-------------------------------------------------------------------------)

200- How does MCTS apply to non-deterministic games like Urban Rivals?

Ans- By extending UCT to manage hidden information and simultaneous moves, improving performance over plain UCT.

(-------------------------------------------------------------------------)

201- What unique aspect does Phantom Chess add to the application of MCTS?

Ans- It incorporates "fog of war," where players only see their own pieces, requiring MCTS to handle partial information effectively.

(-------------------------------------------------------------------------)

202- How does MCTS handle the hidden information in games like Dou Di Zhu?

Ans- By using information sets to store and utilize rollout statistics for indistinguishable game states.

(-------------------------------------------------------------------------)

203- What was a surprising finding in using expectimax versus determinization for strategy fusion in card games?

Ans- Expectimax was found to be more beneficial than having a perfect opponent model.

(-------------------------------------------------------------------------)

204- How is MCTS used in combinatorial optimization problems?

Ans- MCTS guides searches in problems like Mixed Integer Programming (MIP) and Travelling Salesman Problem (TSP) for better solutions.

(-------------------------------------------------------------------------)

205- What advantage does MCTS offer in Mixed Integer Programming (MIP) compared to traditional solvers?

Ans- UCT-based node selection shows promising results compared to traditional solvers like CPLEX.

(-------------------------------------------------------------------------)

206- What approach is used to solve the Travelling Salesman Problem (TSP) with MCTS?

Ans- A nested Monte Carlo search algorithm with time windows is used to address the problem.

(-------------------------------------------------------------------------)

207- How does MCTS apply to the sailing domain problem?

Ans- UCT scales better for increasing problem size than techniques like asynchronous realtime dynamic programming.

(-------------------------------------------------------------------------)

208- In what way does MCTS contribute to constraint satisfaction problems?

Ans- By integrating constraint propagation techniques to enhance performance over random selections.

(-------------------------------------------------------------------------)

209- What is a key benefit of using MCTS for scheduling problems like printer scheduling?

Ans- MCTS with heuristics improves solution quality and efficiency in scheduling tasks.

(-------------------------------------------------------------------------)

210- How does MCTS help in procedural content generation (PCG)?

Ans- It generates a range of good solutions by leveraging its inherent restart mechanism and local iterated search.

(-------------------------------------------------------------------------)

211- What is the application of Feature UCT Selection (FUSE) in feature selection?

Ans- FUSE achieves state-of-the-art performance in reducing features and selecting relevant ones.

(-------------------------------------------------------------------------)

212- What is the limitation of vanilla MCTS?

Ans- It struggles in high-complexity problems without modifications, especially when computational resources are limited.

(-------------------------------------------------------------------------)

213- What is determinization in MCTS?

Ans- It removes uncertainty by assuming fixed values for unknown variables in games with imperfect information.

(-------------------------------------------------------------------------)

214- What is the Master Combination in MCTS?

Ans- It refers to using multiple MCTS modifications together to optimize performance in competitive scenarios.

(-------------------------------------------------------------------------)

215- What is the role of the 'Selection' phase in MCTS?

Ans- It selects the next node to explore based on a tree policy until reaching a leaf node.

(-------------------------------------------------------------------------)

216- What is the 'Expansion' phase in MCTS?

Ans- It adds new nodes to the tree, representing unexplored states or actions.

(-------------------------------------------------------------------------)

217- What happens during the 'Simulation' phase in MCTS?

Ans- A random simulation is performed from the current state to estimate the outcome of the game.

(-------------------------------------------------------------------------)

218- What is 'Backpropagation' in MCTS?

Ans- It updates the values of nodes along the path from the leaf node to the root based on the simulation outcome.

(-------------------------------------------------------------------------)

219- What is the Upper Confidence Bounds for Trees (UCT) formula used for?

Ans- UCT is used to balance exploration and exploitation in MCTS by selecting actions based on both the expected reward and the uncertainty of the action.

(-------------------------------------------------------------------------)

220- How does UCT manage the exploration-exploitation trade-off?

Ans- UCT uses a formula that incorporates both the average value of an action and a term that favors less-explored actions.

(-------------------------------------------------------------------------)

221- What is General Game Playing (GGP) and its relation to MCTS?

Ans- GGP involves creating agents that can play various games without prior knowledge, often relying on MCTS to simulate possible actions.

(-------------------------------------------------------------------------)

222- What is the difference between perfect and imperfect information games?

Ans- In perfect information games, all players have complete knowledge of the game state, while in imperfect information games, some information is hidden from players.

(-------------------------------------------------------------------------)

223- Why is MCTS well-suited for imperfect information games?

Ans- MCTS can handle uncertainty by simulating multiple possibilities and choosing actions that work well across many outcomes.

(-------------------------------------------------------------------------)

224- What is General Video Game Playing (GVGP)?

Ans- GVGP focuses on agents playing various 2D real-time video games with limited decision time, typically using MCTS.

(-------------------------------------------------------------------------)

225- What are the common modifications of MCTS for handling RTS games?

Ans- Handling large branching factors, real-time decisions, and partial observability are key challenges, addressed with variations like belief-state sampling.

(-------------------------------------------------------------------------)

226- What is Thompson Sampling, and how is it related to MCTS?

Ans- Thompson Sampling is a heuristic exploration-exploitation approach based on Bayesian inference, sometimes used as a tree policy in MCTS.

(-------------------------------------------------------------------------)

227- What is EXP3, and why might it be used in MCTS?

Ans- EXP3 is a multi-armed bandit algorithm that doesn't assume any reward distribution, useful for environments with partial observability.

(-------------------------------------------------------------------------)

228- What is the History Heuristic in MCTS?

Ans- It's an enhancement that assumes historically good actions are likely to be good again, storing global statistics to bias action selection.

(-------------------------------------------------------------------------)

229- What is the ε-greedy method in the context of MCTS?

Ans- It's a strategy where the best historical action is chosen with probability ε, and a random action is selected with probability 1 - ε.

(-------------------------------------------------------------------------)

230- How does the History Heuristic bias simulations in MCTS?

Ans- By using global action statistics to preferentially select actions that have performed well historically.

(-------------------------------------------------------------------------)

231- What is the Last-Good Reply Policy (LGRP)?

Ans- An enhancement that stores the best counteractions to opponents' moves globally to improve action selection.

(-------------------------------------------------------------------------)

232- How does LGRP differ from the standard History Heuristic?

Ans- LGRP focuses on storing and utilizing the best replies to specific actions, whereas the History Heuristic tracks overall action success.

(-------------------------------------------------------------------------)

233- What is the N-grams Selection Technique (NST) in MCTS?

Ans- NST stores statistics for sequences of actions (n-grams) to better capture patterns in action effectiveness.

(-------------------------------------------------------------------------)

234- Why might introducing a bias like the History Heuristic improve MCTS performance?

Ans- It can accelerate convergence by focusing on actions that have historically yielded better outcomes.

(-------------------------------------------------------------------------)

235- What is a potential downside of using the History Heuristic in MCTS?

Ans- It may introduce bias that reduces performance in some problems by overlooking less-explored but potentially optimal actions.

(-------------------------------------------------------------------------)

236- How does the Boltzmann Distribution differ from ε-greedy in action selection?

Ans- It selects actions probabilistically based on their estimated values, providing a smoother preference compared to the ε-greedy method.

(-------------------------------------------------------------------------)

237- In what phase of MCTS is LGRP typically used?

Ans- It is used to rank unexplored actions during both the selection and simulation phases.

(-------------------------------------------------------------------------)

238- Why is global storage of action statistics beneficial in MCTS?

Ans- It allows for the accumulation of broader experience, improving the quality of action selection across different states.

(-------------------------------------------------------------------------)

239- What assumption underlies the use of the History Heuristic?

Ans- That the effectiveness of an action is relatively independent of the state in which it's performed.

(-------------------------------------------------------------------------)

240- How does the ε-greedy method balance exploration and exploitation?

Ans- By occasionally selecting random actions (exploration) while mostly choosing the best historical actions (exploitation).

(-------------------------------------------------------------------------)

241- What types of games benefit most from the History Heuristic in MCTS?

Ans- Games where certain actions are generally advantageous regardless of the specific state, like some board and strategy games.

(-------------------------------------------------------------------------)

242- How do n-grams in NST enhance MCTS performance?

Ans- By recognizing and exploiting effective sequences of actions rather than evaluating actions in isolation.

(-------------------------------------------------------------------------)

243- What's the main difference between standard MCTS and History Heuristic-enhanced MCTS?

Ans- The enhanced version uses global action statistics to influence selection, whereas standard MCTS relies on local statistics within the tree.

(-------------------------------------------------------------------------)

244- Why must one be cautious when introducing biases like the History Heuristic into MCTS?

Ans- Because biases can limit exploration and adaptability, potentially leading to suboptimal performance in varied situations.

(-------------------------------------------------------------------------)

245- What is the All-Moves-As-First (AMAF) heuristic?

Ans- AMAF evaluates moves based on how often and how successfully they were played across all simulations, rather than treating each state–action pair independently.

(-------------------------------------------------------------------------)

246- How does Rapid Value Estimation (RAVE) improve MCTS?

Ans- RAVE shares action values across subtrees, increasing the data available for each move, which can speed up convergence but may introduce noise.

(-------------------------------------------------------------------------)

247- What is the role of the weighting parameter in MCTS-RAVE?

Ans- The weighting parameter decays from 1 to 0 as visit counts increase, interpolating between AMAF values and unbiased Monte Carlo values.

(-------------------------------------------------------------------------)

248- What are some limitations of RAVE in games like RTS?

Ans- In RTS games, small changes in nearby unit positions can mislead RAVE's action value estimates, leading to inaccurate assessments.

(-------------------------------------------------------------------------)

249- How does the Maximum Frequency method address non-uniform tree shapes in MCTS?

Ans- It adjusts thresholds used in score recalculations to maximize the difference between optimal and non-optimal moves, refining the decision process.

(-------------------------------------------------------------------------)

250- What is the problem of ‘optimistic actions’ in MCTS, and how is it mitigated?

Ans- Optimistic actions initially seem promising but are refutable. They are mitigated by using a sufficiency threshold that discounts exploration for sufficiently good moves.

(-------------------------------------------------------------------------)

251- What are hybrid MCTS-minimax algorithms, and why are they used?

Ans- These algorithms combine shallow minimax search with MCTS to avoid the pitfalls of averaging outcomes, especially in games with tactical traps.

(-------------------------------------------------------------------------)

252- What is the impact of early termination in MCTS simulations?

Ans- Early termination cuts off simulations at a certain depth and applies minimax-style evaluations, saving time but potentially introducing evaluation uncertainty.

(-------------------------------------------------------------------------)

253- How do heavy playouts improve MCTS performance?

Ans- Heavy playouts incorporate domain knowledge into simulations, reducing the risk of MCTS converging on suboptimal moves.

(-------------------------------------------------------------------------)

254- How does the sufficiency threshold modify the UCT formula in MCTS?

Ans- It replaces the exploration constant with a function that discounts exploration for sufficiently good moves, rearranging move order based on current estimates.

(-------------------------------------------------------------------------)

255- What is the main challenge of applying MCTS to games with imperfect information?

Ans- The hidden information in games increases the branching factor, requiring complex probabilistic handling of game states.

(-------------------------------------------------------------------------)

256- What are the three main challenges of Perfect Information MCTS (PIMC)?

Ans- Strategy fusion, non-locality, and sharing computational budget across multiple trees.

(-------------------------------------------------------------------------)

257- How does Information Set MCTS (ISMCTS) address the challenges of determinization?

Ans- ISMCTS uses information sets to group states indistinguishable to a player, optimizing computational resources and reducing strategy fusion.

(-------------------------------------------------------------------------)

258- What is the role of heavy playouts in MCTS?

Ans- Heavy playouts introduce domain-specific knowledge into the simulation phase, improving the realism and performance of rollouts.

(-------------------------------------------------------------------------)

259- What is the balance required in heavy playouts for MCTS?

Ans- A balance between randomness and heuristics is needed to explore the solution space while incorporating expert knowledge.

(-------------------------------------------------------------------------)

260- How can policy updates improve MCTS performance?

Ans- Policy updates can modify the tree-building or playout phases using techniques like RAVE or heuristics to bias decision-making.

(-------------------------------------------------------------------------)

261- What is the RAVE method in MCTS?

Ans- RAVE (Rapid Action Value Estimation) enhances MCTS by applying independent sampling to update the default policy with action value estimates.

(-------------------------------------------------------------------------)

262- What is the RIDE method in MCTS?

Ans- RIDE (Rapid Incentive Difference Evaluation) updates MCTS policies using pairwise sampling of action values to improve decision-making.

(-------------------------------------------------------------------------)

263- How does Online Outcome Sampling (OOS) improve MCTS in imperfect information games?

Ans- OOS targets relevant parts of the game tree and converges closer to Nash Equilibrium using incremental tree building.

(-------------------------------------------------------------------------)

264- What is the role of opponent modeling in MCTS for imperfect information games?

Ans- Opponent modeling helps predict and simulate the opponent’s strategy, allowing more informed decision-making.

(-------------------------------------------------------------------------)

265- What is strategy fusion in MCTS and why is it a problem?

Ans- Strategy fusion occurs when different strategies are applied inconsistently across determinized states, leading to suboptimal decisions.

(-------------------------------------------------------------------------)

266- What is non-locality in the context of PIMC?

Ans- Non-locality refers to the challenge where optimal decisions in subgames don't necessarily lead to globally optimal outcomes.

(-------------------------------------------------------------------------)

267- How does ISMCTS handle imperfect information more efficiently than PIMC?

Ans- ISMCTS uses information sets to combine similar game states, reducing the effects of strategy fusion and improving resource allocation.

(-------------------------------------------------------------------------)

268- What is a chance node in MCTS for games with imperfect information?

Ans- A chance node represents a point in the game where random events or hidden information (e.g., card draws) affect the state.

(-------------------------------------------------------------------------)

269- How does recursive MCTS (IIMCTS) prevent information leakage in playouts?

Ans- IIMCTS restricts playouts to avoid adapting to hidden information, preventing leakage of private data like an opponent's hand in card games.

(-------------------------------------------------------------------------)

270- What is Dynamic Difficulty Adjustment (DDA) in MCTS?

Ans- DDA dynamically adjusts the difficulty of a game based on the player's skill level to keep the game challenging and engaging.

(-------------------------------------------------------------------------)

271- How is MCTS used to mimic human behavior in games?

Ans- MCTS can be trained with human data to predict and imitate human actions, leading to more human-like playstyles in games like Spades.

(-------------------------------------------------------------------------)

272- How can biasing techniques improve MCTS to emulate human players?

Ans- Biasing the UCT formula with human-trained neural networks helps MCTS better mimic human decision-making patterns.

(-------------------------------------------------------------------------)

273- What major breakthrough did MCTS achieve in the game of Go?

Ans- MCTS was the first algorithm to significantly improve computer Go, later combined with ML in AlphaGo to defeat professional players.

(-------------------------------------------------------------------------)

274- What are the two key ML models used in the AlphaGo approach?

Ans- The policy function and the value function, both modeled with deep convolutional networks.

(-------------------------------------------------------------------------)

275- What is the role of the value function in the AlphaGo architecture?

Ans- It approximates the outcome of the game from a given state, helping predict the game’s result.

(-------------------------------------------------------------------------)

276- What does the policy function do in MCTS when combined with ML?

Ans- It guides action selection by providing probabilities for each action in a given state.

(-------------------------------------------------------------------------)

277- How was AlphaGo's policy function initially trained?

Ans- It was trained through supervised learning over a dataset of expert human moves.

(-------------------------------------------------------------------------)

278- What is the main difference between AlphaGo and AlphaZero in training?

Ans- AlphaGo uses supervised learning initially, while AlphaZero relies purely on reinforcement learning via self-play.

(-------------------------------------------------------------------------)

279- What improvement did AlphaZero bring to the MCTS + ML approach?

Ans- AlphaZero eliminated the need for supervised learning, optimizing purely through self-play reinforcement learning.

(-------------------------------------------------------------------------)

280- What is the role of the rollout in AlphaGo’s MCTS?

Ans- Rollouts help estimate the result of a game from a leaf node during the search phase.

(-------------------------------------------------------------------------)

281- What is the “Big Win Strategy” and how does it relate to MCTS?

Ans- It's an AlphaGo-inspired approach for Othello that uses an additional neural network to estimate point differences and rewards during MCTS.

(-------------------------------------------------------------------------)

282- What enhancement did MoHex-CNN bring to the game of Hex?

Ans- It introduced a deep convolutional network as a policy network combined with MCTS, improving the previous state-of-the-art agent.

(-------------------------------------------------------------------------)

283- How does the DeepEzo approach differ from AlphaGo?

Ans- DeepEzo trains its policy function from the results of agents that don’t use a policy function, rather than through continuous self-improvement.

(-------------------------------------------------------------------------)

284- What innovation did Yang et al. bring to AlphaGo's komi setup in Go?

Ans- They developed networks that dynamically learn the komi (handicap) instead of using a fixed value.

(-------------------------------------------------------------------------)

285- How is the policy network used differently in Gomoku compared to AlphaGo?

Ans- In Gomoku, only one neural network is used to predict the game’s outcome, trained using Adaptive Dynamic Programming (ADP).

(-------------------------------------------------------------------------)

286- What is the role of neural networks in RTS games like StarCraft when combined with MCTS?

Ans- Neural networks are used for strategic-level search decisions, while lower-level tactical decisions are handled by adversarial search.

(-------------------------------------------------------------------------)

287- What is chance event bucketing, and how is it used in Hearthstone with MCTS?

Ans- It groups random outcomes into buckets during simulations to reduce the branching factor in MCTS.

(-------------------------------------------------------------------------)

288- What modification did Świechowski et al. introduce in MCTS for Hearthstone?

Ans- They used a value network for early cutoff after the opponent’s turn and biased simulations using a policy network.

(-------------------------------------------------------------------------)

289- How was MCTS combined with shallow neural networks in the game Dots-and-Boxes?

Ans- A shallow neural network predicts winning chances, guiding the simulation phase of MCTS.

(-------------------------------------------------------------------------)

290- How does pseudo-roulette selection work in Hearthstone’s MCTS algorithm?

Ans- It selects moves either by Boltzmann distribution over heuristic evaluations or chooses the best move with a specific probability.

(-------------------------------------------------------------------------)

291- What two-level hierarchical approach is used in Barriga’s RTS framework with MCTS?

Ans- MCTS is used for strategic actions, followed by adversarial search for lower-level tactical decisions.

(-------------------------------------------------------------------------)

292- What is Temporal Difference (TD) learning in reinforcement learning?

Ans- TD learning is a method where predictions about future rewards are updated based on the difference between predicted and actual rewards.

(-------------------------------------------------------------------------)

293- How does TD learning relate to Monte Carlo Tree Search (MCTS)?

Ans- TD learning updates state values using temporal difference errors, and this approach can replace or complement the traditional Upper Confidence Bound for Trees (UCT) formula in MCTS.

(-------------------------------------------------------------------------)

294- What is the purpose of integrating TD learning into MCTS?

Ans- Integrating TD learning helps improve the accuracy of value estimates during the backpropagation phase and potentially enhances the overall performance of MCTS.

(-------------------------------------------------------------------------)

295- Describe the TD-UCT Single Backup variant.

Ans- TD-UCT Single Backup assumes that future state values have converged to the final reward of the playout, using the TD error based on the distance to the terminal state.

(-------------------------------------------------------------------------)

296- What is the TD-UCT Weighted Rewards variant?

Ans- This variant simplifies the TD-UCT approach by setting specific parameters to make the TD value function fully replace the average score estimate in MCTS while retaining the exploration part of UCT.

(------------------------------------------------------------------------)

297- Explain the TD-UCT Merged Bootstrapping variant.

Ans- TD-UCT Merged Bootstrapping updates state values based on both the immediate reward and the value of the next state, without assuming convergence to the final reward.

(------------------------------------------------------------------------)

298- How does TD learning affect the selection and backpropagation phases in MCTS?

Ans- TD learning modifies the backpropagation phase by updating values using TD estimates and affects the selection phase by using a weighted average of TD and UCT estimates.

(------------------------------------------------------------------------)

299- What role does the TD(λ) algorithm play in MCTS?

Ans- The TD(λ) algorithm updates state values based on immediate rewards and future state values, integrating eligibility traces to refine value estimates during the backpropagation phase.

(------------------------------------------------------------------------)

300- What are some advantages of using TD learning with MCTS?

Ans- Advantages include improved value estimates, faster convergence, better generalization, and the ability to handle complex states that may not be well-represented in simulations.

(------------------------------------------------------------------------)

301- What are the main challenges when combining ML techniques like TD learning with MCTS?

Ans- Challenges include the need for high-quality and large volumes of data, avoiding overfitting, and integrating different programming languages and technologies for ML and game development.

(------------------------------------------------------------------------)

302- How does TD learning improve the simulation phase in MCTS?

Ans- TD learning can adapt the policy used in simulations by evaluating states more accurately and learning from past simulations to enhance performance.

(------------------------------------------------------------------------)

303- Why is evolving heuristic functions beneficial in MCTS?

Ans- Evolving heuristic functions can improve performance by providing more effective evaluation criteria, though it is time-consuming and best done offline.

(------------------------------------------------------------------------)

304- What is the main focus of evolving policies in MCTS?

Ans- To optimize the tree policy and default simulation policy using evolutionary computation.

(------------------------------------------------------------------------)

305- What approach did Lucas et al. (2014) introduce for MCTS policy optimization?

Ans- They used a weight vector to influence both tree and default policies, optimized via a (1 + 1) Evolution Strategy (ES).

(------------------------------------------------------------------------)

306- How are weights used in Lucas et al.'s approach to default policies?

Ans- Weights are assigned to features in the state space to bias actions towards states with higher aggregated weight sums, with a softmax function for exploration.

(------------------------------------------------------------------------)

307- What is KB Fast-Evo MCTS introduced by Perez et al. (2014)?

Ans- An extension of Lucas et al.'s work that dynamically evolves features and biases MCTS simulations using an evolving knowledge base.

(------------------------------------------------------------------------)

308- How does KB Fast-Evo MCTS handle the game environment in GVGP?

Ans- It dynamically extracts features from game states and evolves them to bias MCTS decisions.

(------------------------------------------------------------------------)

309- What is the role of curiosity and experience in KB Fast-Evo MCTS?

Ans- Curiosity and experience are metrics used to calculate knowledge changes, which influence the predicted end-game reward.

(------------------------------------------------------------------------)

310- What game did Pettit and Helmbold (2012) apply MCTS/UCT and Evolution Strategies to optimize?

Ans- They applied it to the abstract strategy game Hex to optimize the default policy.

(------------------------------------------------------------------------)

311- What patterns did Pettit and Helmbold evolve in their system for MCTS simulations?

Ans- Local board patterns and move selection strategies, such as uniform local and tenuki, which improved simulation performance.

(------------------------------------------------------------------------)

312- What did Bravi et al. (2016) aim to evolve in MCTS's tree policy?

Ans- A heuristic function that could potentially perform better than UCB in general video game playing (GVGP).

(------------------------------------------------------------------------)

313- What components did Bravi et al. (2016) use in their evolved heuristic for MCTS?

Ans- They used constants, unary and binary functions, and variables like child visits and parent visits in a syntax tree representation.

(------------------------------------------------------------------------)

314- How did Bravi et al. (2017) extend their previous work on heuristic functions in MCTS?

Ans- They added game-specific variables to evolve heuristic functions, yielding insights about game-specific exploration-exploitation trade-offs.

(------------------------------------------------------------------------)

315- What is the Rolling Horizon Evolutionary Algorithm (RHEA)?

Ans- RHEA is an evolutionary algorithm that evolves sequences of actions for decision-making, commonly used as an alternative to MCTS.

(------------------------------------------------------------------------)

316- How does RHEA differ from MCTS in decision-making?

Ans- Instead of selection and simulation, RHEA evolves action sequences, and only the first action from the best sequence is executed in the game.

(------------------------------------------------------------------------)

317- What is hybridization in the context of MCTS and RHEA?

Ans- Hybridization combines MCTS and RHEA, like running RHEA with rollouts or switching to MCTS after RHEA for alternative paths.

(------------------------------------------------------------------------)

318- What did Baier and Cowling (2018) propose in Evolutionary MCTS (EMCTS)?

Ans- They introduced a variant where nodes contain sequences of actions, and rollouts are replaced by evolving these sequences using mutation operators.

(------------------------------------------------------------------------)

319- What is one of the primary domains of MCTS application beyond games?

Ans- Automated planning, often formulated as Markov Decision Processes (MDP) or Partially Observable MDP (POMDP).

(------------------------------------------------------------------------)

320- Can you give an example of MCTS being used in combinatorial optimization?

Ans- MCTS has been used to optimize Horner’s method for polynomial evaluation and low-latency communication.

(------------------------------------------------------------------------)

321- What simplification methods are used in MCTS for tackling large planning problems?

Ans- Techniques like state-action pair abstractions and dynamic time allocation strategies, as seen in ASAP-UCT.

(------------------------------------------------------------------------)

322- How does PROST enhance MCTS for probabilistic planning?

Ans- PROST incorporates action pruning, search space modification, and Q-value initialization using iterative deepening search (IDS).

(------------------------------------------------------------------------)

323- What is the modification introduced in Convex Hull Monte Carlo Tree Search (CHMCTS)?

Ans- CHMCTS includes multi-objective optimization with multi-dimensional reward vectors, adapting the standard UCT formula.

(------------------------------------------------------------------------)

324- How is MCTS applied to interplanetary trajectory planning?

Ans- MCTS for trajectory planning introduces a unique contraction phase, which prunes subtrees with terminal states.

(------------------------------------------------------------------------)

325- What is the advantage of hierarchical MCTS in planning?

Ans- Hierarchical MCTS decomposes tasks into subtasks, helping manage large search spaces and overcoming the curse of dimensionality.

(------------------------------------------------------------------------)

326- How is MCTS utilized in multi-robot decentralized planning?

Ans- Each robot maintains its own search tree, and the trees are periodically shared, updating joint policies for real-time online replanning.

(------------------------------------------------------------------------)

327- What is the k-Rejection MDP model in MCTS?

Ans- It is a hierarchical MCTS model for emergency planning, where plans can be rejected under certain conditions.

(------------------------------------------------------------------------)

328- What is the 3N-MCTS approach?

Ans- It uses three neural networks—Rollout policy network, In-score filter network, and Expansion policy network—integrated with MCTS.

(------------------------------------------------------------------------)




